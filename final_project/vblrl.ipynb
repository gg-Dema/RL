{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3771e90e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dema/PycharmProjects/VBLRL/lib/python3.10/site-packages/gym/spaces/box.py:127: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  logger.warn(f\"Box bound precision lowered by casting to {self.dtype}\")\n"
     ]
    }
   ],
   "source": [
    "import metaworld\n",
    "import random\n",
    "from collections import defaultdict\n",
    "from network import BNN\n",
    "from planner import Planner\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d6437af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dema/PycharmProjects/VBLRL/lib/python3.10/site-packages/gym/spaces/box.py:127: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  logger.warn(f\"Box bound precision lowered by casting to {self.dtype}\")\n"
     ]
    }
   ],
   "source": [
    "ml10 = metaworld.ML10()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "460cc211",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BayesianLayer(nn.Module):\n",
    "    # cit https://github.com/Harry24k/bayesian-neural-network-pytorch\n",
    "    def __init__(self, input_size, output_size, deterministic=False):\n",
    "        \n",
    "        super(BayesianLayer).__init__()\n",
    "        \n",
    "        # CLASS PARAMETERS\n",
    "        # ---------------------------\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.W_shape = (input_size, output_size)\n",
    "        self.b_shape = (output_size)\n",
    "        self.deterministic = deterministic\n",
    "        \n",
    "        # PARAMETERS (W, b)\n",
    "        #-----------------------------\n",
    "        # mean\n",
    "        self.W_mu = torch.normal(torch.zeros(self.W_shape), torch.ones(self.W_shape))\n",
    "        self.b_mu = torch.normal(torch.zeros(self.b_shape), torch.ones(self.b_shape))\n",
    "        # variance \n",
    "        # ps: sono partito dalla varianza per ottenere il sigma, \n",
    "        # il codice originale fa il contrario (usando np.exp, log etc)\n",
    "        self.W_sigma = torch.full(self.W_shape, 0.5)\n",
    "        self.b_sigma = torch.full(self.b_shape, 0.5)\n",
    "        \n",
    "        # log(variance)   | rho in original codeBase\n",
    "        self.W_log_sigma = torch.log(self.W_sigma)\n",
    "        self.b_log_sigma = torch.log(self.b_sigma)\n",
    "        \n",
    "        # variational inference ---> compute update respect previous params\n",
    "        self.W_mu_old = torch.Tensor(self.W_shape).detach()\n",
    "        self.b_mu_old = torch.Tensor(self.b_shape).detach()\n",
    "        self.W_sigma_old = torch.Tensor(self.W_shape).detach()\n",
    "        self.b_sigma_old = torch.Tensor(self.b_shape).detach()\n",
    "        self.W_log_sigma_old = torch.Tensor(self.W_shape).detach()\n",
    "        self.b_log_sigma_old = torch.Tensor(self.b_shape).detach()\n",
    "        \n",
    "        # copy value of previous parameter\n",
    "        self.save_old_parameters()\n",
    "        \n",
    "        \n",
    "    def save_old_parameters(self): \n",
    "        self.W_mu_old = self.W_mu.clone()\n",
    "        self.W_log_sigma_old = self.W_log_sigma.clone()\n",
    "        self.b_mu_old = self.b_mu.clone()\n",
    "        self.b_log_sigma_old = self.b_log_sigma.clone()\n",
    "        \n",
    "    def forward(self, X): \n",
    "        # link github method\n",
    "        \n",
    "        #sample W and b, return linear\n",
    "        #weight = self.W_mu + torch.exp(self.W_log_sigma)*torch.randn_like(self.W_log_sigma)\n",
    "        #bias = self.b_mu + torch.exp(self.b_log_sigma)*torch.randn_like(self.b_log_sigma)\n",
    "        #return F.linear(X, weight, bias)\n",
    "        \n",
    "        gamma = X @self.W_mu + self.b_mu\n",
    "        delta = X.pow(2)@ self.W_sigma + self.b_sigma\n",
    "        # implement output as normal distrib result ???\n",
    "        # \" local reparametrization tricks\"\n",
    "    \n",
    "    # this 2 method looks like set the random sample to a fixed value--> no perturbation\n",
    "    def freeze(): pass\n",
    "    def unfreeze(): pass\n",
    "    def extra_repr():  pass #just return parameters\n",
    "    \n",
    "    def parameter_number(): pass\n",
    "    # the 2 next as the same doc-string\n",
    "    def get_parameters(): pass\n",
    "    def get_old_parameters(): pass\n",
    "    def set_parameters(): pass\n",
    "    def save_old_parameters(): pass\n",
    "    \n",
    "class BNN(nn.Module): \n",
    "    # input = observation + action\n",
    "    # output mean(reward, next_state), var(reward, next_state)\n",
    "    \n",
    "    def __init__(self, obs_shape, action_shape): pass\n",
    "    def get_parameters(self) -> dict: pass\n",
    "    def load_parameters(self, parameters): pass\n",
    "    def forward(self, x): pass  # x = obs+act \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b87ae773",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "import numpy as np\n",
    "\n",
    "class CEM: \n",
    "    def __init__(self, \n",
    "                 cost_function, \n",
    "                 solution_dim, max_iters, \n",
    "                 pop_size=2, elite_size=1, \n",
    "                 upper_bound=None, lower_bound=None, \n",
    "                 epsilon=0.001, alpha=0.25\n",
    "                ): \n",
    "        \n",
    "        self.solution_dim = solution_dim\n",
    "        self.pop_size = pop_size\n",
    "        self.elite_size = elite_size\n",
    "        \n",
    "        self.cost_function = cost_function\n",
    "        self.upper_bound = upper_bound\n",
    "        self.lower_bound = lower_bound\n",
    "\n",
    "        self.max_iters = max_iters\n",
    "        self.epsilon = epsilon\n",
    "        self.alpha = alpha\n",
    "        \n",
    "        assert(elite_size < popsize)\n",
    "        \n",
    "        \n",
    "    def solve(self, init_mean, init_var): \n",
    "        '''optimize the cost funct'''\n",
    "        mean, var, time = init_mean, init_var, 0\n",
    "        # pr distrib, truncated normal\n",
    "        distrib = stats.truncnorm(-2, 2, loc=np.zeros_like(mean), scale=np.ones_like(var))\n",
    "    \n",
    "        while (t<self.max_iters) and np.max(var) > self.epsilon: \n",
    "            \n",
    "            lower_bound_distrib = mean - self.lower_bound\n",
    "            upper_bound_distrib = self.upper_bound - mean \n",
    "            constrained_var = np.minimum(\n",
    "                np.minimum(np.square(lower_bound_distrib/2), np.square(upper_bound_distrib/2))\n",
    "                var\n",
    "            )\n",
    "            \n",
    "            samples = distrib.rvs(size=[self.pop_size, self.solution_dim]) * np.sqrt(constrained_var) + mean\n",
    "            samples = samples.astype(np.float32)\n",
    "            costs = self.cost_function(samples)\n",
    "            elite = samples[np.argsort(-costs)][:self.elite_size]\n",
    "\n",
    "            new_mean = np.mean(elite, axis=0)\n",
    "            new_var  np.var(elite, axis=0)\n",
    "            \n",
    "            # just for fun, what if we set a decay in the alpha?\n",
    "            # consider in the end more important the previous solution ??\n",
    "            mean = self.alpha * mean + (1 - self.alpha)*new_mean\n",
    "            var = self.alpha * var + (1 - self.alpha) * new_var\n",
    "            \n",
    "            time += 1\n",
    "            \n",
    "        return mean \n",
    "    \n",
    "class Planner:\n",
    "    \n",
    "    def __init__(self, world_distrib_model, task_nets): \n",
    "        \n",
    "        # params\n",
    "        self.action_dim = action_dim\n",
    "        self.plan_horizion = plan_horizon\n",
    "        self.current_env_id = None\n",
    "        self.system_current_state = None\n",
    "        # ------------------------------\n",
    "        # nets and optimizer\n",
    "        self.world = world_distrib_model\n",
    "        self.task_nets = task_nets\n",
    "        self.optimizer = CEM(\n",
    "            cost_function=self.cost_function \n",
    "            solution_dim =self.plan_horizon * self.action_dim, \n",
    "            max_iters=5, \n",
    "            pop_size=200, elite_size=50, \n",
    "            # shape upper/lower bound : solution dim = act*horizon (1D)\n",
    "            upper_bound=np.tile(np.ones(self.action_dim), [self.plan_horizon])\n",
    "            lower_bound=np.tile(-np.ones(self.action_dim), [self.plan_horizon]), \n",
    "            alpha=0.1)\n",
    "        # ------------------------------\n",
    "        \n",
    "        # solution init:\n",
    "        self.action_buffer = np.array([]).reshape(0, self.action_dim)\n",
    "        self.prev_solution = np.tile(np.zeros(self.action_dim), [self.plan_horizion])\n",
    "        self.init_var = np.tile(np.ones(self.action_dim)*1.5, [self.plan_horizion])\n",
    "        # ------------------------------\n",
    "    \n",
    "    \n",
    "    # \"action\" part\n",
    "    def get_action(self, obs, env_index): # call to self.optimizer.solve\n",
    "        self.current_env_id = env_index\n",
    "        \n",
    "        #Planning part : if not planning return random act (with np.random.uniform(-1, 1, solution_dim))\n",
    "        # aggiungere dopo , casomai\n",
    "        \n",
    "        # se ho già generato una sequenza d'azione, ritorno la prima di queste\n",
    "        if self.action_buffer.shape[0] > 0: \n",
    "            action, self.action_buffer = self.action_buffer[0], self.action_buffer[1:]\n",
    "            return action\n",
    "    \n",
    "        # generate plan: \n",
    "        self.system_current_state = obs\n",
    "        solution = self.optimizer.solve(mean=self.prev_sol, \n",
    "                                        var=self.init_var)\n",
    "        \n",
    "        self.prev_solution = np.concatenate(\n",
    "            [np.copy(solution)[self.action_dim:], np.zeros(self.action_dim)]\n",
    "        )\n",
    "        self.action_buffer = solution[:self.action_dim](-1, self.action_dim)\n",
    "        return self.get_action(obs, env_idx)\n",
    "    \n",
    "    def get_weight_as_vect(self) -> np.array: pass\n",
    "    def restore_weight_from_vect(self): pass\n",
    "    \n",
    "    \n",
    "    \n",
    "    # equivalent to cost funct\n",
    "    def rollout(self, env, action_sequence):\n",
    "        # not work, should keep a env id inside?\n",
    "        assert(env==self.current_env_id)\n",
    "        # num op --> shape action sequence[0]\n",
    "        action_seq.view(-1, self.plan_horizion, self.action_dim)\n",
    "        \n",
    "        # current observation \n",
    "        current_obs = ...\n",
    "        # prepare rewards = zeros\n",
    "        for step in range(self.plan_horizion): \n",
    "            \n",
    "            current_action = action_sequence[t]\n",
    "            \n",
    "            dynamic_output = self.task_nets[self.current_env_id].infer(current_observation, current_action)\n",
    "            reward, reward_mu, reward_var, new_state, new_state_mu, new_state_var = dynamic_output\n",
    "            \n",
    "            # sum up reward for each state\n",
    "            # in this part there is a lot of stuff about reshape, transpose etc\n",
    "            \n",
    "        return -rewards\n",
    "            \n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "240b50a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    # incomplete as funct list ---> reset, use index etc\n",
    "    # ragno usa namedTuple, try to understand why\n",
    "    def __init__(self, memory_size=50000): pass\n",
    "    def sample_batch(self, batch_size=32): pass\n",
    "    def append(self, s, a, r, d, new_s): pass\n",
    "    def remaining_space(self): pass\n",
    "    def used_space(self): pass\n",
    "\n",
    "class MultiTargetReplayBuffer: \n",
    "    #set of replay buffer over same env, different config of task\n",
    "    def __init__(self, memory_size_for_buffer, env, task): pass\n",
    "    def append(self, task, s, a, r, d, new_s): pass\n",
    "    def random_batch(self, task): pass\n",
    "    def load_buffer_from_path(self, task, path): pass\n",
    "    # manca clear_buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d53e8876",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VBLRL_alg: \n",
    "    \n",
    "    def __init__(self, agent, world_model, task_specific): pass\n",
    "    def init_task_specific_from_world(task_id): pass\n",
    "    def update_buffer(): pass \n",
    "    def update_world_model( buffers_data ): pass\n",
    "    def update_task_specific( buffer_data): pass\n",
    "    \n",
    "    def run(): \n",
    "        pass\n",
    "        for task_id in range(len(tasks)): \n",
    "            init_task_specific_from_world(task_id)\n",
    "            \n",
    "            \n",
    "            # NON SO UN CAZZO \n",
    "            \n",
    "            for i in range(iteration): \n",
    "                for t in range(max_time_horizion): \n",
    "                    agent.get_action() # how set parameters\n",
    "                    \n",
    "    def train(self): \n",
    "        for env_id in range(self.env_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3695240",
   "metadata": {},
   "source": [
    "# LAUNCHER ml10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "838c4365",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_env(): \n",
    "    \"\"\"create 10 env with random task (random as init condition)\"\"\"\n",
    "    envs_name = list(ml10.train_classes.keys())\n",
    "    tasks_for_env = defaultdict(list)\n",
    "    \n",
    "    for task in ml10.train_tasks: \n",
    "        tasks_for_env[task.env_name].append(task)\n",
    "    \n",
    "    envs = []\n",
    "    for env_name in envs_name: \n",
    "        env = ml10.train_classes[env_name]()\n",
    "        env.set_task(tasks_for_env[env_name][random.randint(0, 49)])\n",
    "        envs.append(env)\n",
    "    \n",
    "    return envs\n",
    "\n",
    "def get_NN_for_exp(num_task, obs_shape, action_shape): \n",
    "    world_distrib = BNN(obs_shape, action_shape)\n",
    "    task_specific_BNNs = [BNN(obs_shape, action_shape) for _ in range(num_task)]\n",
    "    return world_distrib, task_specific_BNNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4cc27ad7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dema/PycharmProjects/VBLRL/lib/python3.10/site-packages/gym/spaces/box.py:127: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  logger.warn(f\"Box bound precision lowered by casting to {self.dtype}\")\n"
     ]
    }
   ],
   "source": [
    "envs = prepare_env()\n",
    "observation_dim = envs[0].observation_space.shape[0]\n",
    "action_dim = envs[0].action_space.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "58468288",
   "metadata": {},
   "outputs": [],
   "source": [
    "world_distrib, task_specific_BNN = get_NN_for_exp(len(envs), observation_dim, action_dim)\n",
    "planner = Planner(world_distrib, task_specific_BNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4421d518",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "VBLRL",
   "language": "python",
   "name": "vblrl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
