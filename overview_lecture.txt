Lecture 1 : DECISION MARKOV PROCESSES
	Overview in markov process
		(S, A, T, R) 
		interacts agent and environment
		def h_t = (o0, a0, r1, o1, a1, ... rt, ot, at)
		state s_t = funct(h_t)
	Markov assumption 
		the state is indipendet form past
		p(s_t+1|s_t,a_t) = p(s_t+1|h_t,a_t)
		HOW MARKOVITION PROCESS: 
			set history = state
			OR
			set observation = state ---> sufficient in most case
	REWARD:	
		scalar signal 
		funct of state, action 
		COST = 1 - reward
		Stochastic or deterministic
	DETERMINISTIC ENVIRONMENT: 
		transiction funct, reward and action are table
	POLICY: funct that map all the state to action 
		deterministic policy --> a = ùúã(s)
		stochastic policy -----> ùúã(s | a) ~ probability distrib 
	
	Trajectory probability: 
		according to a generic policy ùúã, calc pr of pass to a 
		particular sequence of state (and action)
		‚Ñôùúã(s0,a0,...st,at)
		|
		= ùúã(a0|s0)p(s1|s0,a0)ùúã(a1|s1)p(s2|s1,a1)...p(st|st-1,at-1)ùúã(at|st)
		
	Infinite horizon discounted setting:
		If the goal it's maximaize the reward, any inf setting
		will lead to a inf reward 
		- add parameter to MDP formulation: gamma Œ≥ in [0, 1]
			0 = watch only actual reward
			1 = infinitive reward
			
		Use it in value and action funct.
	Value function : how good is a state. Estimate the value of any state respect the future
		Vùúã(st) = 
			ùîºùúã[rt+Œ≥rt+1+Œ≥2rt+2+Œ≥3rt+3+...|st] =
			ùîº[‚ÖÄ‚àû (Œ≥^h*r_h) | (s_0 = s_t), (a_h=ùúã(s_h)) ,(s_h+1‚àºp(.|s_h,a_h))]
			
	Q-Function: estimate how good is a action given a particular policy,
		    use a particular action and then follow the policy pi
		    Qùúã(s_t, a_t) = ùîº[‚ÖÄ‚àû Œ≥^h *r_h |
		    				(s0,a0)=(st,at),
		    				a_h+1=ùúã(sh),
		    				s_h+1‚àºp(.|s_h,a_h)]
		    notice the presence of a_h+1 as next move
	
	Bellman Equation: expand V and Q funct in terms of next value, recursively 
		Vùúã(st) = rt+Œ≥ùîº[Vùúã(s‚Äô)]	  | 
		Qùúã(st,a) = rt+Œ≥ùîº[Vùúã(s‚Äô)] |
		The difference between the two funct is in the ùîº --> 
			one is given a (Q), the other (V) is given a PrDistrib for a = ùúã(s)
	Optimal Policy : ùúã* only if = Vùúã*(s)‚â•Vùúã(s)
	
	Bellman optimality: 
		V*(s) = max(a) [r(s,a)+Œ≥ùîº (s‚Äô~p(.|s,a)) V*(s‚Äô)]
		V*(s) = max(a) Q*(s, a)
		
		using ùúã^ = argmax (a) Q*(s, a) 
	
		TH (1) : This implies ùúã*=argmax(a) Q*(s,a) is an optimal policy
		TH (2) : leaving the remaining ‚Äúproblem‚Äù to V(s‚Äô)

		
Lecture 2 : VALUE ITERATION 

	Policy Evaluation
		Exact evaluation ---> bellman give us a set of contraints
				      use matrix form for extract Value for each state
				      ( O(s^3) ) ---> non fattibile
		Math iteration: 
			
			stuff for future understanding: 
				fixed point ---> point such that f(x) = x
					find it by iteration --> until any x_i+1 = f(x_i) in small range
				Contractions funct: 
					mapping f:M->M = admit convergence to fixed point
					Riduce distance between point
					| f(x) - f(x') | <= k |x - x'| for k in [0, 1)
				
				Contraction Operator: 
					basic form: matrix O
					Cool form : Bellman operator
			
		ITERATIVE POLICY EVALUATION: 
			------ | just calc value for each state
			V_0 init in [0, 1/1-Œ≥]
			V_i+1 = R + Œ≥*P*V_i ----> O(s^2)
			
			TH(1): proof of convergence --->  norm as contraction operator
				||Vt(s)-Vùúã(s)|| ‚â§ Œ≥t||V0-Vùúã||
			
			# of iteration --> O(S^2 * ln(1/œµ)) with œµ error
			
			BEST POLICY BY BELLMAN OPTIMALITY ---> bellman operator B or T
			B(V) = max(a) ( r(s, a) + Œ≥ùîº[V(s') <--- non linear, max
			BELLMAN OP FOR Q-FUNCT
			T(Q)(s,a) = r(s,a)+Œ≥ùîº[max(a)[Q(s', a')]
		
		VALUE ITERATION: 
			Obtain Q* as TQ* ---> Q* is a fixed point
			Init ||Q_0|| in range [0, 1/(1-Œ≥)]
			update until convergence: Q_i+1 = TQ_i
		
			Extract policy ---> ùúã_i(s) = argmax(a) Q_i(s,a)
				with error = 2Œ≥^i/(1-Œ≥) ||Q0-Q*|| ‚â§ œµ
		
			
Lecture 3 : POLICY ITERATION 
			
Lecture 4 : LQR, iLQR, MDP

	INTRO Finite-Horizon MDP
	|	add term H, mu_0 ---> H = horizon size (timing) > 0
	|						  mu_0 = probability distrib for init state
	|						  		sample here S_0
	|	Add time dependent policy ---> ùúã0 = policy for time step = 0
	|									the policy now can be different in the same state,
	|									depends of time of state s_t
	|									[USED ALSO FOR CHANGE BEHAVIOUR, EXPLORE ENV AT T=0]
	|	New version of V and Q:
	|		V_h (s) = ùîº[‚ÖÄ(h, h-1) r(s, a)] respect s_h = s, 
	|												a_h = ùúã_h(s_h),
	|												s_h+1 ~ P(.|s_h, a_h)
	|  ---------------------------------------------------------------------------------
	|		Q_h(s) = ùîº[‚ÖÄ(h, h-1) r(s, a)] respect s_h = s, 
	|												a_h = a,
	|												a_h+1= ùúã_h+1(s_h)
	|												s_h+1 ~ P(.|s_h, a_h)
	|		_______________________________________________________________________
	|	BELLMAN EQ FOR FINITE HORIZON
	|		Q_h(s) = r(s, a) + ùîº[V_h+1(s')] with expect s' ~ p(.|s, a)
	|		use it for optimal policy
	|			ùúã = {ùúã0*, ùúã1*, ..., ùúã_H-1*}
	|			
	|			Q_H-1 * (s, a) it's easy to calc, the expectation is 0
	|			So, go backward until state s_t=0 for find trajectory
	|			start from Q_H-1 = r(s, a)
	|				so best policy ---> ùúã*(s) = argmax(a) Q_H-1 * (s, a) 
	|									V*(s) H_1 = max(Q*_H-1 (s, a) 
	|									they are the same formula in this case
	|				go backward to H-2 and so on 
	\\\\\\\\\\\
	
	INTRO TO CONTROL PROBLEMS
		start with cartpole problem
		---> continuos state, made by 
				[angular pos, angular vel, linear pos, linear vel] 				= x
		---> action = control: force applied (to the pole in this case)			= u
		---> goal = minimize the cost AKA (-Reward) 							= g(x, u)
					
					[min cost = max reward]
					[min ùîº_ùúã[c_H(x_H) + ‚ÖÄ(over h-->H-1) c_h(x_h,u_h)]
					extract policy by value iteration
					u_h = ùúã(x_h) where x_0 ~ form mu_0 ---> init distrib
					
					PROBLEM: WE HAVE TO DISCRETIZE THE SPACE
					|	opt 1: quadrettatura, square of dim œµ
					|		   total # of square = (1/œµ)^d + (1/œµ)^k
					|		   d = # state variable
					|		   k = # input
					|		   ------------------------
					|		   EXIST CURSE OF DIMENSIONALITY ---> infeasible for state dim ~5 or 6
					VS
					|	opt 2: work directly in continuos space
					|	SEE LINEAR SYSTEM
					\\\\\\\\\\
	LINEAR SYS
		x_t+1 = A*x_t + B*u_t ---> base state form
								   equivalent to a transiction funct
		define cost funct: 
			QUADRATIC COST FUNCT ----> for min of funct
				c(x_t, u_t) 
					
	
		
	
			

Lecture 5 :  
Lecture 6 : 

