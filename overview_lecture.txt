Lecture 1 : DECISION MARKOV PROCESSES
	Overview in markov process
		(S, A, T, R) 
		interacts agent and environment
		def h_t = (o0, a0, r1, o1, a1, ... rt, ot, at)
		state s_t = funct(h_t)
	Markov assumption 
		the state is indipendet form past
		p(s_t+1|s_t,a_t) = p(s_t+1|h_t,a_t)
		HOW MARKOVITION PROCESS: 
			set history = state
			OR
			set observation = state ---> sufficient in most case
	REWARD:	
		scalar signal 
		funct of state, action 
		COST = 1 - reward
		Stochastic or deterministic
	DETERMINISTIC ENVIRONMENT: 
		transiction funct, reward and action are table
	POLICY: funct that map all the state to action 
		deterministic policy --> a = ğœ‹(s)
		stochastic policy -----> ğœ‹(s | a) ~ probability distrib 
	
	Trajectory probability: 
		according to a generic policy ğœ‹, calc pr of pass to a 
		particular sequence of state (and action)
		â„™ğœ‹(s0,a0,...st,at)
		|
		= ğœ‹(a0|s0)p(s1|s0,a0)ğœ‹(a1|s1)p(s2|s1,a1)...p(st|st-1,at-1)ğœ‹(at|st)
		
	Infinite horizon discounted setting:
		If the goal it's maximaize the reward, any inf setting
		will lead to a inf reward 
		- add parameter to MDP formulation: gamma Î³ in [0, 1]
			0 = watch only actual reward
			1 = infinitive reward
			
		Use it in value and action funct.
	Value function : how good is a state. Estimate the value of any state respect the future
		Vğœ‹(st) = 
			ğ”¼ğœ‹[rt+Î³rt+1+Î³2rt+2+Î³3rt+3+...|st] =
			ğ”¼[â…€âˆ (Î³^h*r_h) | (s_0 = s_t), (a_h=ğœ‹(s_h)) ,(s_h+1âˆ¼p(.|s_h,a_h))]
			
	Q-Function: estimate how good is a action given a particular policy,
		    use a particular action and then follow the policy pi
		    Qğœ‹(s_t, a_t) = ğ”¼[â…€âˆ Î³^h *r_h |
		    				(s0,a0)=(st,at),
		    				a_h+1=ğœ‹(sh),
		    				s_h+1âˆ¼p(.|s_h,a_h)]
		    notice the presence of a_h+1 as next move
	
	Bellman Equation: expand V and Q funct in terms of next value, recursively 
		Vğœ‹(st) = rt+Î³ğ”¼[Vğœ‹(sâ€™)]	  | 
		Qğœ‹(st,a) = rt+Î³ğ”¼[Vğœ‹(sâ€™)] |
		The difference between the two funct is in the ğ”¼ --> 
			one is given a (Q), the other (V) is given a PrDistrib for a = ğœ‹(s)
	Optimal Policy : ğœ‹* only if = Vğœ‹*(s)â‰¥Vğœ‹(s)
	
	Bellman optimality: 
		V*(s) = max(a) [r(s,a)+Î³ğ”¼ (sâ€™~p(.|s,a)) V*(sâ€™)]
		V*(s) = max(a) Q*(s, a)
		
		using ğœ‹^ = argmax (a) Q*(s, a) 
	
		TH (1) : This implies ğœ‹*=argmax(a) Q*(s,a) is an optimal policy
		TH (2) : leaving the remaining â€œproblemâ€ to V(sâ€™)

		
Lecture 2 : VALUE ITERATION 

	Policy Evaluation
		Exact evaluation ---> bellman give us a set of contraints
				      use matrix form for extract Value for each state
				      ( O(s^3) ) ---> non fattibile
		Math iteration: 
			
			stuff for future understanding: 
				fixed point ---> point such that f(x) = x
					find it by iteration --> until any x_i+1 = f(x_i) in small range
				Contractions funct: 
					mapping f:M->M = admit convergence to fixed point
					Riduce distance between point
					| f(x) - f(x') | <= k |x - x'| for k in [0, 1)
				
				Contraction Operator: 
					basic form: matrix O
					Cool form : Bellman operator
			
		ITERATIVE POLICY EVALUATION: 
			------ | just calc value for each state
			V_0 init in [0, 1/1-Î³]
			V_i+1 = R + Î³*P*V_i ----> O(s^2)
			
			TH(1): proof of convergence --->  norm as contraction operator
				||Vt(s)-Vğœ‹(s)|| â‰¤ Î³t||V0-Vğœ‹||
			
			# of iteration --> O(S^2 * ln(1/Ïµ)) with Ïµ error
			
			BEST POLICY BY BELLMAN OPTIMALITY ---> bellman operator B or T
			B(V) = max(a) ( r(s, a) + Î³ğ”¼[V(s') <--- non linear, max
			BELLMAN OP FOR Q-FUNCT
			T(Q)(s,a) = r(s,a)+Î³ğ”¼[max(a)[Q(s', a')]
		
		VALUE ITERATION: 
			Obtain Q* as TQ* ---> Q* is a fixed point
			Init ||Q_0|| in range [0, 1/(1-Î³)]
			update until convergence: Q_i+1 = TQ_i
		
			Extract policy ---> ğœ‹_i(s) = argmax(a) Q_i(s,a)
				with error = 2Î³^i/(1-Î³) ||Q0-Q*|| â‰¤ Ïµ
		
			
Lecture 3 : POLICY ITERATION 
			
Lecture 4 : LQR, iLQR, MDP

	INTRO Finite-Horizon MDP
	|	add term H, mu_0 ---> H = horizon size (timing) > 0
	|						  mu_0 = probability distrib for init state
	|						  		sample here S_0
	|	Add time dependent policy ---> ğœ‹0 = policy for time step = 0
	|									the policy now can be different in the same state,
	|									depends of time of state s_t
	|									[USED ALSO FOR CHANGE BEHAVIOUR, EXPLORE ENV AT T=0]
	|	New version of V and Q:
	|		V_h (s) = ğ”¼[â…€(h, h-1) r(s, a)] respect s_h = s, 
	|												a_h = ğœ‹_h(s_h),
	|												s_h+1 ~ P(.|s_h, a_h)
	|  ---------------------------------------------------------------------------------
	|		Q_h(s) = ğ”¼[â…€(h, h-1) r(s, a)] respect s_h = s, 
	|												a_h = a,
	|												a_h+1= ğœ‹_h+1(s_h)
	|												s_h+1 ~ P(.|s_h, a_h)
	|		_______________________________________________________________________
	|	BELLMAN EQ FOR FINITE HORIZON
	|		Q_h(s) = r(s, a) + ğ”¼[V_h+1(s')] with expect s' ~ p(.|s, a)
	|		use it for optimal policy
	|			ğœ‹ = {ğœ‹0*, ğœ‹1*, ..., ğœ‹_H-1*}
	|			
	|			Q_H-1 * (s, a) it's easy to calc, the expectation is 0
	|			So, go backward until state s_t=0 for find trajectory
	|			start from Q_H-1 = r(s, a)
	|				so best policy ---> ğœ‹*(s) = argmax(a) Q_H-1 * (s, a) 
	|									V*(s) H_1 = max(Q*_H-1 (s, a) 
	|									they are the same formula in this case
	|				go backward to H-2 and so on 
	\\\\\\\\\\\
	
	INTRO TO CONTROL PROBLEMS
		start with cartpole problem
		---> continuos state, made by 
				[angular pos, angular vel, linear pos, linear vel] 				= x
		---> action = control: force applied (to the pole in this case)			= u
		---> goal = minimize the cost AKA (-Reward) 							= g(x, u)
					
					[min cost = max reward]
					[min ğ”¼_ğœ‹[c_H(x_H) + â…€(over h-->H-1) c_h(x_h,u_h)]
					extract policy by value iteration
					u_h = ğœ‹(x_h) where x_0 ~ form mu_0 ---> init distrib
					
					PROBLEM: WE HAVE TO DISCRETIZE THE SPACE
					|	opt 1: quadrettatura, square of dim Ïµ
					|		   total # of square = (1/Ïµ)^d + (1/Ïµ)^k
					|		   d = # state variable
					|		   k = # input
					|		   ------------------------
					|		   EXIST CURSE OF DIMENSIONALITY ---> infeasible for state dim ~5 or 6
					VS
					|	opt 2: work directly in continuos space
					|	SEE LINEAR SYSTEM
					\\\\\\\\\\
	LINEAR SYS
		x_t+1 = A*x_t + B*u_t ---> base state form
								   equivalent to a transiction funct
		define cost funct: 
			QUADRATIC COST FUNCT ----> for min of funct 
			EQUIVALENT TO V FUNCT 
				c(x_t, u_t)  = (x.T Q x) + (u.T R u)   
					Q = cost for each state	  [dim k*k]
					R = cost for each action  [dim r*r] 
					see https://stanford.edu/class/ee363/lectures/dlqr.pdf for more details
					zero-controll = Allways non zero-cost
			Q_FUNC with cost funct
			Q*_h (x, u) = c(x, u) + ğ”¼[V_h+1(x')] where x' ~ p(.|x, u)
			
				expansion Q*_h:
					c(x, u) = (x.T Q x) + (u.T R u)
					x' ~ p(.|x, u) are deterministic ---> x_t+1 = A*x_t + B*u_t
					Total : (x.T*Q*xt) + (u.T*R*u) + V*_h+1(Ax + Bu)
		
		Inductive step: Given assumption V_h+1(x) = x.T*P_h+1*x |
						prove that V_h(x) = x.T*P_h*x			|---> different time step
			
			
		
	
			

Lecture 5 :  
Lecture 6 : 

