{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "DdrijM8lTmTk"
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g_n6KlxA4Eev",
    "outputId": "ba34922e-b866-4051-8b5d-e234c56b2040"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "a_xSOAC54Q-T"
   },
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Grayscale(),\n",
    "    transforms.Resize(64),\n",
    "    transforms.ToTensor(), \n",
    "])\n",
    "dataset = datasets.ImageFolder(root='./drive/MyDrive/data_rollouts2', transform=transform)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w858XzYRC-zO",
    "outputId": "b27cd8b2-9edb-40bb-f43b-7b83fad7f98f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(219, 7000)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataloader), len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "CYDo5Hhue6as"
   },
   "outputs": [],
   "source": [
    "batch = next(iter(dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iE6ERtn7fFuH",
    "outputId": "471e7aef-1385-4b5b-9bcc-2cf19d2f50ea"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 64, 64])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data, _ = batch[0], batch[1]\n",
    "data[0].shape # ---> 1channel, 64x64pixel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "3FdOOo6iAPHm"
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "\n",
    "class Flatten(nn.Module):\n",
    "    def forward(self, input):\n",
    "        return input.view(input.shape[0], -1)\n",
    "\n",
    "class UnFlatten(nn.Module):\n",
    "    def forward(self, input, size=1024):\n",
    "        return input.view(input.size(0), size, 1, 1)\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, image_channels=1, h_dim=1024, z_dim=32):\n",
    "        super(VAE, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(image_channels, 32, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 256, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),  #--> [256, 2, 2] \n",
    "            Flatten()\n",
    "        ).to(device)\n",
    "        \n",
    "        self.fc1 = nn.Linear(h_dim, z_dim)\n",
    "        self.fc2 = nn.Linear(h_dim, z_dim)\n",
    "        self.fc3 = nn.Linear(z_dim, h_dim)\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            UnFlatten(),\n",
    "            nn.ConvTranspose2d(h_dim, 128, kernel_size=5, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=5, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=6, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, image_channels, kernel_size=6, stride=2),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "        \n",
    "    def sampling_trick(self, mu, logvar):\n",
    "        std = logvar.mul(0.5).exp_().to(device)\n",
    "        esp = torch.randn(mu.size()).to(device)\n",
    "        z = mu + std * esp\n",
    "        return z\n",
    "    \n",
    "    def encode(self, x):\n",
    "        h = self.encoder(x) # 256x4 per singola img\n",
    "        # mu, log_var = self.fc1(h.view(-1)), self.fc2(h.view(-1)) OK per singola img\n",
    "        mu, log_var = self.fc1(h), self.fc2(h) # batch mode\n",
    "        z = self.sampling_trick(mu, log_var)\n",
    "        return z, mu, log_var\n",
    "\n",
    "    def decode(self, z):\n",
    "        z = self.fc3(z)\n",
    "        z = self.decoder(z)\n",
    "        return z\n",
    "\n",
    "    def forward(self, x):\n",
    "        z, mu, logvar = self.encode(x)\n",
    "        z = self.decode(z)\n",
    "        return z, mu, logvar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "cX3JO8d-PUh5"
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def loss_fn(predicted, original, mu, log_var):\n",
    "  BCE = F.binary_cross_entropy(predicted, original, size_average=False)\n",
    "  KLD = -0.5 * torch.mean(1 + log_var - mu.pow(2) - log_var.exp())\n",
    "  return BCE+KLD, BCE, KLD\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "mCY2SLeCQUcS"
   },
   "outputs": [],
   "source": [
    "model = VAE().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "rOFOsg1oT-HR"
   },
   "outputs": [],
   "source": [
    "# just for testing (single img)\n",
    "from PIL import Image\n",
    "img = Image.open('/content/drive/MyDrive/rollouts/CarRacing_random1/car_0_001.jpg')\n",
    "img = transform(img).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "QKmr-1uaesbP"
   },
   "outputs": [],
   "source": [
    "# just for testing (single img) \n",
    "h = model.encoder(img)\n",
    "mu, log_var = model.fc1(h.view(-1)), model.fc2(h.view(-1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "hsOW4D_dQAAk",
    "outputId": "dad84708-b666-4e1d-d015-05efbc2ca11b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[0] sample[0/219] Loss: 92699.688, 92699.688, 0.000\n",
      "Epoch[0] sample[1/219] Loss: 92762.109, 92762.109, 0.001\n",
      "Epoch[0] sample[2/219] Loss: 91424.750, 91424.734, 0.014\n",
      "Epoch[0] sample[3/219] Loss: 91065.398, 91065.297, 0.104\n",
      "Epoch[0] sample[4/219] Loss: 90430.320, 90430.289, 0.033\n",
      "Epoch[0] sample[5/219] Loss: 88792.914, 88792.898, 0.012\n",
      "Epoch[0] sample[6/219] Loss: 88266.758, 88266.750, 0.008\n",
      "Epoch[0] sample[7/219] Loss: 87953.797, 87953.789, 0.009\n",
      "Epoch[0] sample[8/219] Loss: 88106.250, 88106.234, 0.015\n",
      "Epoch[0] sample[9/219] Loss: 87172.930, 87172.891, 0.037\n",
      "Epoch[0] sample[10/219] Loss: 83913.633, 83913.523, 0.113\n",
      "Epoch[0] sample[11/219] Loss: 84888.141, 84887.797, 0.346\n",
      "Epoch[0] sample[12/219] Loss: 82538.344, 82537.859, 0.483\n",
      "Epoch[0] sample[13/219] Loss: 82642.188, 82641.617, 0.573\n",
      "Epoch[0] sample[14/219] Loss: 81759.148, 81758.469, 0.676\n",
      "Epoch[0] sample[15/219] Loss: 81568.031, 81567.180, 0.854\n",
      "Epoch[0] sample[16/219] Loss: 80399.078, 80397.906, 1.169\n",
      "Epoch[0] sample[17/219] Loss: 81155.031, 81153.828, 1.202\n",
      "Epoch[0] sample[18/219] Loss: 80239.797, 80238.875, 0.921\n",
      "Epoch[0] sample[19/219] Loss: 80353.258, 80352.547, 0.714\n",
      "Epoch[0] sample[20/219] Loss: 82024.031, 82023.578, 0.450\n",
      "Epoch[0] sample[21/219] Loss: 78616.555, 78615.508, 1.049\n",
      "Epoch[0] sample[22/219] Loss: 78280.008, 78278.656, 1.351\n",
      "Epoch[0] sample[23/219] Loss: 76410.430, 76408.586, 1.848\n",
      "Epoch[0] sample[24/219] Loss: 76927.719, 76926.516, 1.204\n",
      "Epoch[0] sample[25/219] Loss: 77706.789, 77705.367, 1.419\n",
      "Epoch[0] sample[26/219] Loss: 76870.234, 76869.062, 1.171\n",
      "Epoch[0] sample[27/219] Loss: 77614.789, 77613.734, 1.056\n",
      "Epoch[0] sample[28/219] Loss: 79215.977, 79214.906, 1.073\n",
      "Epoch[0] sample[29/219] Loss: 79191.164, 79189.969, 1.193\n",
      "Epoch[0] sample[30/219] Loss: 80135.922, 80135.328, 0.597\n",
      "Epoch[0] sample[31/219] Loss: 76990.953, 76989.891, 1.063\n",
      "Epoch[0] sample[32/219] Loss: 75995.711, 75994.156, 1.554\n",
      "Epoch[0] sample[33/219] Loss: 78279.500, 78277.641, 1.862\n",
      "Epoch[0] sample[34/219] Loss: 78509.984, 78508.922, 1.065\n",
      "Epoch[0] sample[35/219] Loss: 76984.641, 76983.516, 1.125\n",
      "Epoch[0] sample[36/219] Loss: 76773.312, 76772.133, 1.177\n",
      "Epoch[0] sample[37/219] Loss: 77796.875, 77795.711, 1.163\n",
      "Epoch[0] sample[38/219] Loss: 77774.156, 77772.930, 1.227\n",
      "Epoch[0] sample[39/219] Loss: 74758.164, 74756.094, 2.073\n",
      "Epoch[0] sample[40/219] Loss: 78888.602, 78887.617, 0.985\n",
      "Epoch[0] sample[41/219] Loss: 77556.922, 77555.812, 1.106\n",
      "Epoch[0] sample[42/219] Loss: 77301.984, 77300.688, 1.301\n",
      "Epoch[0] sample[43/219] Loss: 77562.328, 77561.016, 1.311\n",
      "Epoch[0] sample[44/219] Loss: 76135.344, 76133.344, 1.998\n",
      "Epoch[0] sample[45/219] Loss: 73959.281, 73956.828, 2.449\n",
      "Epoch[0] sample[46/219] Loss: 75935.461, 75933.484, 1.979\n",
      "Epoch[0] sample[47/219] Loss: 73083.266, 73080.414, 2.850\n",
      "Epoch[0] sample[48/219] Loss: 76487.594, 76485.781, 1.812\n",
      "Epoch[0] sample[49/219] Loss: 76540.016, 76537.969, 2.048\n",
      "Epoch[0] sample[50/219] Loss: 75735.352, 75733.078, 2.275\n",
      "Epoch[0] sample[51/219] Loss: 77028.320, 77026.203, 2.115\n",
      "Epoch[0] sample[52/219] Loss: 74948.781, 74945.703, 3.078\n",
      "Epoch[0] sample[53/219] Loss: 76147.000, 76144.516, 2.481\n",
      "Epoch[0] sample[54/219] Loss: 75454.109, 75451.266, 2.840\n",
      "Epoch[0] sample[55/219] Loss: 74929.367, 74926.164, 3.207\n",
      "Epoch[0] sample[56/219] Loss: 72711.898, 72707.750, 4.147\n",
      "Epoch[0] sample[57/219] Loss: 78121.547, 78119.406, 2.137\n",
      "Epoch[0] sample[58/219] Loss: 73628.156, 73623.797, 4.361\n",
      "Epoch[0] sample[59/219] Loss: 77784.266, 77781.750, 2.515\n",
      "Epoch[0] sample[60/219] Loss: 75287.219, 75283.219, 4.003\n",
      "Epoch[0] sample[61/219] Loss: 77426.203, 77423.375, 2.827\n",
      "Epoch[0] sample[62/219] Loss: 76932.609, 76929.422, 3.187\n",
      "Epoch[0] sample[63/219] Loss: 74082.312, 74076.961, 5.354\n",
      "Epoch[0] sample[64/219] Loss: 76855.883, 76851.953, 3.930\n",
      "Epoch[0] sample[65/219] Loss: 72560.398, 72555.102, 5.296\n",
      "Epoch[0] sample[66/219] Loss: 75321.898, 75317.742, 4.156\n",
      "Epoch[0] sample[67/219] Loss: 71484.805, 71477.969, 6.837\n",
      "Epoch[0] sample[68/219] Loss: 75560.094, 75555.633, 4.465\n",
      "Epoch[0] sample[69/219] Loss: 75050.445, 75045.727, 4.722\n",
      "Epoch[0] sample[70/219] Loss: 70925.062, 70918.195, 6.864\n",
      "Epoch[0] sample[71/219] Loss: 74983.250, 74977.984, 5.268\n",
      "Epoch[0] sample[72/219] Loss: 76341.273, 76337.266, 4.008\n",
      "Epoch[0] sample[73/219] Loss: 76551.812, 76547.953, 3.857\n",
      "Epoch[0] sample[74/219] Loss: 74442.328, 74436.766, 5.565\n",
      "Epoch[0] sample[75/219] Loss: 71436.586, 71429.109, 7.479\n",
      "Epoch[0] sample[76/219] Loss: 73231.430, 73226.156, 5.275\n",
      "Epoch[0] sample[77/219] Loss: 74216.289, 74211.578, 4.709\n",
      "Epoch[0] sample[78/219] Loss: 76119.078, 76115.266, 3.813\n",
      "Epoch[0] sample[79/219] Loss: 77909.125, 77905.859, 3.262\n",
      "Epoch[0] sample[80/219] Loss: 73953.453, 73947.891, 5.565\n",
      "Epoch[0] sample[81/219] Loss: 74612.484, 74608.062, 4.422\n",
      "Epoch[0] sample[82/219] Loss: 76074.102, 76070.250, 3.848\n",
      "Epoch[0] sample[83/219] Loss: 74580.742, 74575.688, 5.057\n",
      "Epoch[0] sample[84/219] Loss: 74427.695, 74422.406, 5.285\n",
      "Epoch[0] sample[85/219] Loss: 71488.352, 71482.914, 5.436\n",
      "Epoch[0] sample[86/219] Loss: 76637.648, 76634.016, 3.635\n",
      "Epoch[0] sample[87/219] Loss: 75983.867, 75979.141, 4.730\n",
      "Epoch[0] sample[88/219] Loss: 74556.789, 74552.148, 4.641\n",
      "Epoch[0] sample[89/219] Loss: 77992.562, 77989.734, 2.829\n",
      "Epoch[0] sample[90/219] Loss: 74972.461, 74967.609, 4.854\n",
      "Epoch[0] sample[91/219] Loss: 76023.352, 76019.656, 3.694\n",
      "Epoch[0] sample[92/219] Loss: 73148.461, 73143.516, 4.944\n",
      "Epoch[0] sample[93/219] Loss: 74219.930, 74215.211, 4.717\n",
      "Epoch[0] sample[94/219] Loss: 72003.555, 71997.438, 6.119\n",
      "Epoch[0] sample[95/219] Loss: 74177.258, 74172.266, 4.989\n",
      "Epoch[0] sample[96/219] Loss: 74626.812, 74622.766, 4.047\n",
      "Epoch[0] sample[97/219] Loss: 71359.703, 71353.680, 6.024\n",
      "Epoch[0] sample[98/219] Loss: 73740.016, 73735.188, 4.832\n",
      "Epoch[0] sample[99/219] Loss: 72299.453, 72293.188, 6.264\n",
      "Epoch[0] sample[100/219] Loss: 75556.625, 75552.734, 3.887\n",
      "Epoch[0] sample[101/219] Loss: 75167.820, 75163.375, 4.444\n",
      "Epoch[0] sample[102/219] Loss: 75966.156, 75962.500, 3.660\n",
      "Epoch[0] sample[103/219] Loss: 73806.383, 73800.914, 5.470\n",
      "Epoch[0] sample[104/219] Loss: 72152.953, 72146.398, 6.555\n",
      "Epoch[0] sample[105/219] Loss: 71396.031, 71389.180, 6.852\n",
      "Epoch[0] sample[106/219] Loss: 75485.586, 75481.375, 4.213\n",
      "Epoch[0] sample[107/219] Loss: 76258.336, 76253.969, 4.368\n",
      "Epoch[0] sample[108/219] Loss: 74541.695, 74536.625, 5.067\n",
      "Epoch[0] sample[109/219] Loss: 76103.438, 76099.352, 4.084\n",
      "Epoch[0] sample[110/219] Loss: 74076.656, 74072.031, 4.628\n",
      "Epoch[0] sample[111/219] Loss: 76972.352, 76968.938, 3.412\n",
      "Epoch[0] sample[112/219] Loss: 74738.719, 74733.156, 5.566\n",
      "Epoch[0] sample[113/219] Loss: 76074.805, 76070.633, 4.168\n",
      "Epoch[0] sample[114/219] Loss: 75333.688, 75329.109, 4.576\n",
      "Epoch[0] sample[115/219] Loss: 76748.602, 76744.938, 3.666\n",
      "Epoch[0] sample[116/219] Loss: 75860.383, 75855.484, 4.898\n",
      "Epoch[0] sample[117/219] Loss: 74820.375, 74815.109, 5.268\n",
      "Epoch[0] sample[118/219] Loss: 75735.445, 75731.375, 4.073\n",
      "Epoch[0] sample[119/219] Loss: 73575.984, 73570.203, 5.785\n",
      "Epoch[0] sample[120/219] Loss: 74751.172, 74745.672, 5.500\n",
      "Epoch[0] sample[121/219] Loss: 75005.820, 75000.648, 5.171\n",
      "Epoch[0] sample[122/219] Loss: 74362.922, 74357.328, 5.594\n",
      "Epoch[0] sample[123/219] Loss: 72792.750, 72786.406, 6.346\n",
      "Epoch[0] sample[124/219] Loss: 73345.617, 73339.945, 5.673\n",
      "Epoch[0] sample[125/219] Loss: 72792.211, 72786.391, 5.819\n",
      "Epoch[0] sample[126/219] Loss: 72672.727, 72665.547, 7.177\n",
      "Epoch[0] sample[127/219] Loss: 71979.156, 71971.984, 7.171\n",
      "Epoch[0] sample[128/219] Loss: 74881.500, 74876.242, 5.259\n",
      "Epoch[0] sample[129/219] Loss: 75641.484, 75636.938, 4.547\n",
      "Epoch[0] sample[130/219] Loss: 71941.000, 71933.664, 7.340\n",
      "Epoch[0] sample[131/219] Loss: 75774.445, 75769.273, 5.172\n",
      "Epoch[0] sample[132/219] Loss: 75419.547, 75415.125, 4.419\n",
      "Epoch[0] sample[133/219] Loss: 76515.344, 76511.375, 3.966\n",
      "Epoch[0] sample[134/219] Loss: 75752.461, 75748.359, 4.101\n",
      "Epoch[0] sample[135/219] Loss: 74794.047, 74789.078, 4.972\n",
      "Epoch[0] sample[136/219] Loss: 74936.344, 74930.664, 5.680\n",
      "Epoch[0] sample[137/219] Loss: 74178.805, 74172.953, 5.849\n",
      "Epoch[0] sample[138/219] Loss: 75831.461, 75826.844, 4.620\n",
      "Epoch[0] sample[139/219] Loss: 78987.312, 78984.062, 3.249\n",
      "Epoch[0] sample[140/219] Loss: 74962.555, 74957.562, 4.995\n",
      "Epoch[0] sample[141/219] Loss: 76874.695, 76870.391, 4.302\n",
      "Epoch[0] sample[142/219] Loss: 73540.352, 73532.930, 7.420\n",
      "Epoch[0] sample[143/219] Loss: 70775.148, 70767.094, 8.058\n",
      "Epoch[0] sample[144/219] Loss: 72458.422, 72451.562, 6.863\n",
      "Epoch[0] sample[145/219] Loss: 75833.297, 75828.031, 5.269\n",
      "Epoch[0] sample[146/219] Loss: 77405.664, 77402.000, 3.665\n",
      "Epoch[0] sample[147/219] Loss: 74026.000, 74020.227, 5.774\n",
      "Epoch[0] sample[148/219] Loss: 72313.633, 72306.578, 7.056\n",
      "Epoch[0] sample[149/219] Loss: 74909.500, 74904.828, 4.675\n",
      "Epoch[0] sample[150/219] Loss: 75216.906, 75212.227, 4.679\n",
      "Epoch[0] sample[151/219] Loss: 75836.945, 75831.828, 5.118\n",
      "Epoch[0] sample[152/219] Loss: 71323.383, 71316.828, 6.556\n",
      "Epoch[0] sample[153/219] Loss: 75134.992, 75129.836, 5.159\n",
      "Epoch[0] sample[154/219] Loss: 73816.164, 73810.406, 5.758\n",
      "Epoch[0] sample[155/219] Loss: 72575.102, 72569.891, 5.213\n",
      "Epoch[0] sample[156/219] Loss: 75872.961, 75868.703, 4.254\n",
      "Epoch[0] sample[157/219] Loss: 72550.273, 72544.031, 6.241\n",
      "Epoch[0] sample[158/219] Loss: 76413.844, 76409.266, 4.574\n",
      "Epoch[0] sample[159/219] Loss: 73025.906, 73020.570, 5.333\n",
      "Epoch[0] sample[160/219] Loss: 76029.539, 76024.461, 5.076\n",
      "Epoch[0] sample[161/219] Loss: 75492.820, 75488.422, 4.398\n",
      "Epoch[0] sample[162/219] Loss: 73806.312, 73800.781, 5.530\n",
      "Epoch[0] sample[163/219] Loss: 73307.375, 73300.688, 6.685\n",
      "Epoch[0] sample[164/219] Loss: 75805.172, 75800.445, 4.723\n",
      "Epoch[0] sample[165/219] Loss: 70908.844, 70901.812, 7.035\n",
      "Epoch[0] sample[166/219] Loss: 77251.141, 77247.094, 4.048\n",
      "Epoch[0] sample[167/219] Loss: 73108.047, 73102.297, 5.749\n",
      "Epoch[0] sample[168/219] Loss: 75479.836, 75475.766, 4.067\n",
      "Epoch[0] sample[169/219] Loss: 75833.656, 75828.438, 5.221\n",
      "Epoch[0] sample[170/219] Loss: 73424.352, 73418.898, 5.457\n",
      "Epoch[0] sample[171/219] Loss: 74296.664, 74292.500, 4.163\n",
      "Epoch[0] sample[172/219] Loss: 74289.266, 74283.812, 5.452\n",
      "Epoch[0] sample[173/219] Loss: 72702.859, 72696.125, 6.736\n",
      "Epoch[0] sample[174/219] Loss: 74694.422, 74689.195, 5.228\n",
      "Epoch[0] sample[175/219] Loss: 75218.039, 75213.977, 4.062\n",
      "Epoch[0] sample[176/219] Loss: 77259.859, 77256.227, 3.631\n",
      "Epoch[0] sample[177/219] Loss: 72134.312, 72127.984, 6.326\n",
      "Epoch[0] sample[178/219] Loss: 76509.867, 76506.219, 3.652\n",
      "Epoch[0] sample[179/219] Loss: 73511.859, 73506.891, 4.967\n",
      "Epoch[0] sample[180/219] Loss: 73071.547, 73065.930, 5.619\n",
      "Epoch[0] sample[181/219] Loss: 77115.016, 77111.391, 3.625\n",
      "Epoch[0] sample[182/219] Loss: 76146.422, 76142.406, 4.013\n",
      "Epoch[0] sample[183/219] Loss: 73595.734, 73590.062, 5.669\n",
      "Epoch[0] sample[184/219] Loss: 74746.422, 74741.008, 5.412\n",
      "Epoch[0] sample[185/219] Loss: 75270.219, 75265.805, 4.417\n",
      "Epoch[0] sample[186/219] Loss: 75322.516, 75317.812, 4.706\n",
      "Epoch[0] sample[187/219] Loss: 73168.227, 73160.953, 7.271\n",
      "Epoch[0] sample[188/219] Loss: 73695.438, 73688.977, 6.458\n",
      "Epoch[0] sample[189/219] Loss: 73754.047, 73748.258, 5.788\n",
      "Epoch[0] sample[190/219] Loss: 74903.047, 74897.703, 5.346\n",
      "Epoch[0] sample[191/219] Loss: 74375.500, 74369.438, 6.064\n",
      "Epoch[0] sample[192/219] Loss: 77375.688, 77371.719, 3.968\n",
      "Epoch[0] sample[193/219] Loss: 71599.672, 71592.062, 7.612\n",
      "Epoch[0] sample[194/219] Loss: 73895.828, 73889.609, 6.216\n",
      "Epoch[0] sample[195/219] Loss: 76088.969, 76084.016, 4.951\n",
      "Epoch[0] sample[196/219] Loss: 75410.641, 75405.508, 5.132\n",
      "Epoch[0] sample[197/219] Loss: 72907.086, 72899.016, 8.072\n",
      "Epoch[0] sample[198/219] Loss: 72570.195, 72564.234, 5.963\n",
      "Epoch[0] sample[199/219] Loss: 76366.250, 76361.953, 4.301\n",
      "Epoch[0] sample[200/219] Loss: 71494.211, 71486.953, 7.258\n",
      "Epoch[0] sample[201/219] Loss: 74696.070, 74690.344, 5.730\n",
      "Epoch[0] sample[202/219] Loss: 76203.180, 76198.672, 4.508\n",
      "Epoch[0] sample[203/219] Loss: 72185.820, 72179.828, 5.989\n",
      "Epoch[0] sample[204/219] Loss: 72681.234, 72674.562, 6.671\n",
      "Epoch[0] sample[205/219] Loss: 74130.359, 74124.812, 5.543\n",
      "Epoch[0] sample[206/219] Loss: 75440.906, 75435.891, 5.014\n",
      "Epoch[0] sample[207/219] Loss: 75251.273, 75244.969, 6.308\n",
      "Epoch[0] sample[208/219] Loss: 76561.555, 76557.008, 4.545\n",
      "Epoch[0] sample[209/219] Loss: 74870.883, 74863.781, 7.102\n",
      "Epoch[0] sample[210/219] Loss: 74408.734, 74402.859, 5.876\n",
      "Epoch[0] sample[211/219] Loss: 74791.688, 74786.852, 4.836\n",
      "Epoch[0] sample[212/219] Loss: 73730.742, 73725.000, 5.745\n",
      "Epoch[0] sample[213/219] Loss: 74170.781, 74164.547, 6.235\n",
      "Epoch[0] sample[214/219] Loss: 73229.953, 73224.062, 5.891\n",
      "Epoch[0] sample[215/219] Loss: 76178.234, 76173.688, 4.546\n",
      "Epoch[0] sample[216/219] Loss: 73809.938, 73803.398, 6.541\n",
      "Epoch[0] sample[217/219] Loss: 75431.039, 75425.094, 5.948\n",
      "Epoch[0] sample[218/219] Loss: 54540.133, 54533.172, 6.962\n",
      "Epoch[1] sample[0/219] Loss: 73760.047, 73753.391, 6.659\n",
      "Epoch[1] sample[1/219] Loss: 74056.938, 74050.688, 6.254\n",
      "Epoch[1] sample[2/219] Loss: 76176.688, 76172.078, 4.612\n",
      "Epoch[1] sample[3/219] Loss: 75274.023, 75269.000, 5.022\n",
      "Epoch[1] sample[4/219] Loss: 75181.242, 75176.312, 4.926\n",
      "Epoch[1] sample[5/219] Loss: 75185.250, 75180.250, 4.998\n",
      "Epoch[1] sample[6/219] Loss: 73423.336, 73417.711, 5.625\n",
      "Epoch[1] sample[7/219] Loss: 73375.102, 73367.430, 7.669\n",
      "Epoch[1] sample[8/219] Loss: 72990.398, 72983.547, 6.849\n",
      "Epoch[1] sample[9/219] Loss: 72093.688, 72087.734, 5.952\n",
      "Epoch[1] sample[10/219] Loss: 71250.562, 71242.766, 7.799\n",
      "Epoch[1] sample[11/219] Loss: 75142.523, 75137.516, 5.008\n",
      "Epoch[1] sample[12/219] Loss: 75105.383, 75100.688, 4.694\n",
      "Epoch[1] sample[13/219] Loss: 75790.305, 75785.531, 4.774\n",
      "Epoch[1] sample[14/219] Loss: 71449.758, 71440.609, 9.147\n",
      "Epoch[1] sample[15/219] Loss: 73605.758, 73598.594, 7.163\n",
      "Epoch[1] sample[16/219] Loss: 73918.062, 73911.438, 6.624\n",
      "Epoch[1] sample[17/219] Loss: 74667.594, 74661.781, 5.815\n",
      "Epoch[1] sample[18/219] Loss: 75413.836, 75407.906, 5.932\n",
      "Epoch[1] sample[19/219] Loss: 73703.477, 73697.992, 5.486\n",
      "Epoch[1] sample[20/219] Loss: 72581.570, 72575.031, 6.535\n",
      "Epoch[1] sample[21/219] Loss: 75229.688, 75224.664, 5.027\n",
      "Epoch[1] sample[22/219] Loss: 73934.453, 73928.117, 6.332\n",
      "Epoch[1] sample[23/219] Loss: 71920.211, 71913.938, 6.272\n",
      "Epoch[1] sample[24/219] Loss: 75687.766, 75683.102, 4.666\n",
      "Epoch[1] sample[25/219] Loss: 74570.453, 74564.406, 6.049\n",
      "Epoch[1] sample[26/219] Loss: 75901.094, 75896.539, 4.558\n",
      "Epoch[1] sample[27/219] Loss: 73302.320, 73296.648, 5.671\n",
      "Epoch[1] sample[28/219] Loss: 74772.867, 74767.125, 5.743\n",
      "Epoch[1] sample[29/219] Loss: 74181.914, 74175.914, 6.002\n",
      "Epoch[1] sample[30/219] Loss: 74374.148, 74369.391, 4.759\n",
      "Epoch[1] sample[31/219] Loss: 76005.344, 76001.039, 4.304\n",
      "Epoch[1] sample[32/219] Loss: 75207.867, 75202.414, 5.452\n",
      "Epoch[1] sample[33/219] Loss: 71560.977, 71553.766, 7.211\n",
      "Epoch[1] sample[34/219] Loss: 74057.312, 74050.164, 7.149\n",
      "Epoch[1] sample[35/219] Loss: 76751.969, 76748.031, 3.941\n",
      "Epoch[1] sample[36/219] Loss: 74128.602, 74120.984, 7.617\n",
      "Epoch[1] sample[37/219] Loss: 73779.883, 73774.945, 4.941\n",
      "Epoch[1] sample[38/219] Loss: 75314.516, 75308.953, 5.561\n",
      "Epoch[1] sample[39/219] Loss: 71303.547, 71296.828, 6.718\n",
      "Epoch[1] sample[40/219] Loss: 74739.562, 74734.359, 5.203\n",
      "Epoch[1] sample[41/219] Loss: 73945.430, 73940.344, 5.089\n",
      "Epoch[1] sample[42/219] Loss: 72830.828, 72824.812, 6.019\n",
      "Epoch[1] sample[43/219] Loss: 72553.531, 72546.484, 7.045\n",
      "Epoch[1] sample[44/219] Loss: 75195.828, 75191.203, 4.621\n",
      "Epoch[1] sample[45/219] Loss: 73468.125, 73462.797, 5.327\n",
      "Epoch[1] sample[46/219] Loss: 74085.289, 74079.477, 5.811\n",
      "Epoch[1] sample[47/219] Loss: 75514.336, 75509.703, 4.633\n",
      "Epoch[1] sample[48/219] Loss: 72227.859, 72221.445, 6.414\n",
      "Epoch[1] sample[49/219] Loss: 76091.531, 76087.086, 4.445\n",
      "Epoch[1] sample[50/219] Loss: 75210.703, 75205.781, 4.924\n",
      "Epoch[1] sample[51/219] Loss: 73485.781, 73479.672, 6.106\n",
      "Epoch[1] sample[52/219] Loss: 71116.156, 71107.562, 8.595\n",
      "Epoch[1] sample[53/219] Loss: 73777.867, 73772.203, 5.666\n",
      "Epoch[1] sample[54/219] Loss: 74394.742, 74389.016, 5.725\n",
      "Epoch[1] sample[55/219] Loss: 74666.672, 74661.383, 5.286\n",
      "Epoch[1] sample[56/219] Loss: 73616.773, 73609.812, 6.963\n",
      "Epoch[1] sample[57/219] Loss: 76460.141, 76454.656, 5.482\n",
      "Epoch[1] sample[58/219] Loss: 75144.438, 75139.367, 5.067\n",
      "Epoch[1] sample[59/219] Loss: 74788.352, 74782.352, 6.003\n",
      "Epoch[1] sample[60/219] Loss: 74453.695, 74447.688, 6.007\n",
      "Epoch[1] sample[61/219] Loss: 75409.844, 75404.391, 5.453\n",
      "Epoch[1] sample[62/219] Loss: 74847.359, 74840.578, 6.782\n",
      "Epoch[1] sample[63/219] Loss: 72956.703, 72950.109, 6.591\n",
      "Epoch[1] sample[64/219] Loss: 76631.039, 76626.359, 4.677\n",
      "Epoch[1] sample[65/219] Loss: 75884.281, 75879.438, 4.841\n",
      "Epoch[1] sample[66/219] Loss: 68900.891, 68891.656, 9.234\n",
      "Epoch[1] sample[67/219] Loss: 73247.477, 73240.891, 6.584\n",
      "Epoch[1] sample[68/219] Loss: 73276.844, 73269.562, 7.283\n",
      "Epoch[1] sample[69/219] Loss: 72879.711, 72872.172, 7.539\n",
      "Epoch[1] sample[70/219] Loss: 75795.156, 75790.109, 5.049\n",
      "Epoch[1] sample[71/219] Loss: 71575.812, 71569.172, 6.642\n",
      "Epoch[1] sample[72/219] Loss: 73027.734, 73021.453, 6.280\n",
      "Epoch[1] sample[73/219] Loss: 74840.508, 74834.734, 5.777\n",
      "Epoch[1] sample[74/219] Loss: 73070.133, 73064.320, 5.814\n",
      "Epoch[1] sample[75/219] Loss: 74494.648, 74489.672, 4.978\n",
      "Epoch[1] sample[76/219] Loss: 72715.719, 72709.766, 5.956\n",
      "Epoch[1] sample[77/219] Loss: 74025.031, 74019.203, 5.831\n",
      "Epoch[1] sample[78/219] Loss: 72394.414, 72388.211, 6.204\n",
      "Epoch[1] sample[79/219] Loss: 76395.047, 76390.719, 4.328\n",
      "Epoch[1] sample[80/219] Loss: 69617.055, 69607.781, 9.275\n",
      "Epoch[1] sample[81/219] Loss: 77217.211, 77212.219, 4.992\n",
      "Epoch[1] sample[82/219] Loss: 75547.391, 75541.812, 5.581\n",
      "Epoch[1] sample[83/219] Loss: 73361.242, 73355.961, 5.281\n",
      "Epoch[1] sample[84/219] Loss: 75902.617, 75897.734, 4.885\n",
      "Epoch[1] sample[85/219] Loss: 73247.539, 73240.852, 6.684\n",
      "Epoch[1] sample[86/219] Loss: 76704.383, 76699.188, 5.193\n",
      "Epoch[1] sample[87/219] Loss: 74365.992, 74360.320, 5.670\n",
      "Epoch[1] sample[88/219] Loss: 72823.820, 72816.516, 7.307\n",
      "Epoch[1] sample[89/219] Loss: 74754.977, 74750.000, 4.976\n",
      "Epoch[1] sample[90/219] Loss: 75114.906, 75108.914, 5.996\n",
      "Epoch[1] sample[91/219] Loss: 75625.789, 75620.609, 5.179\n",
      "Epoch[1] sample[92/219] Loss: 76553.125, 76548.742, 4.383\n",
      "Epoch[1] sample[93/219] Loss: 74762.859, 74757.109, 5.748\n",
      "Epoch[1] sample[94/219] Loss: 74912.508, 74906.625, 5.886\n",
      "Epoch[1] sample[95/219] Loss: 72330.000, 72323.031, 6.972\n",
      "Epoch[1] sample[96/219] Loss: 76224.836, 76220.320, 4.517\n",
      "Epoch[1] sample[97/219] Loss: 73390.531, 73385.234, 5.295\n",
      "Epoch[1] sample[98/219] Loss: 74220.258, 74215.359, 4.899\n",
      "Epoch[1] sample[99/219] Loss: 74697.227, 74691.047, 6.183\n",
      "Epoch[1] sample[100/219] Loss: 74912.141, 74906.320, 5.822\n",
      "Epoch[1] sample[101/219] Loss: 75294.453, 75288.852, 5.599\n",
      "Epoch[1] sample[102/219] Loss: 72056.547, 72050.672, 5.874\n",
      "Epoch[1] sample[103/219] Loss: 75039.961, 75034.797, 5.166\n",
      "Epoch[1] sample[104/219] Loss: 72960.469, 72953.766, 6.700\n",
      "Epoch[1] sample[105/219] Loss: 70238.695, 70230.406, 8.290\n",
      "Epoch[1] sample[106/219] Loss: 74755.180, 74750.172, 5.010\n",
      "Epoch[1] sample[107/219] Loss: 75924.133, 75919.766, 4.370\n",
      "Epoch[1] sample[108/219] Loss: 73089.812, 73084.000, 5.811\n",
      "Epoch[1] sample[109/219] Loss: 74228.008, 74222.203, 5.806\n",
      "Epoch[1] sample[110/219] Loss: 72611.727, 72605.797, 5.928\n",
      "Epoch[1] sample[111/219] Loss: 73281.672, 73275.570, 6.104\n",
      "Epoch[1] sample[112/219] Loss: 69944.188, 69935.617, 8.574\n",
      "Epoch[1] sample[113/219] Loss: 75179.648, 75174.938, 4.711\n",
      "Epoch[1] sample[114/219] Loss: 73931.312, 73925.266, 6.043\n",
      "Epoch[1] sample[115/219] Loss: 73720.453, 73714.641, 5.816\n",
      "Epoch[1] sample[116/219] Loss: 68547.102, 68539.430, 7.669\n",
      "Epoch[1] sample[117/219] Loss: 75552.211, 75547.594, 4.618\n",
      "Epoch[1] sample[118/219] Loss: 72796.469, 72791.406, 5.059\n",
      "Epoch[1] sample[119/219] Loss: 74159.312, 74154.133, 5.176\n",
      "Epoch[1] sample[120/219] Loss: 72946.812, 72940.125, 6.691\n",
      "Epoch[1] sample[121/219] Loss: 75986.664, 75982.000, 4.665\n",
      "Epoch[1] sample[122/219] Loss: 73340.102, 73334.359, 5.739\n",
      "Epoch[1] sample[123/219] Loss: 74282.492, 74276.312, 6.183\n",
      "Epoch[1] sample[124/219] Loss: 70225.148, 70219.047, 6.104\n",
      "Epoch[1] sample[125/219] Loss: 74669.602, 74664.812, 4.786\n",
      "Epoch[1] sample[126/219] Loss: 74403.203, 74397.734, 5.470\n",
      "Epoch[1] sample[127/219] Loss: 68147.812, 68140.320, 7.495\n",
      "Epoch[1] sample[128/219] Loss: 74672.742, 74668.047, 4.696\n",
      "Epoch[1] sample[129/219] Loss: 69639.484, 69631.844, 7.641\n",
      "Epoch[1] sample[130/219] Loss: 72494.539, 72489.633, 4.910\n",
      "Epoch[1] sample[131/219] Loss: 71322.625, 71315.766, 6.860\n",
      "Epoch[1] sample[132/219] Loss: 73872.680, 73866.219, 6.458\n",
      "Epoch[1] sample[133/219] Loss: 74482.742, 74477.953, 4.792\n",
      "Epoch[1] sample[134/219] Loss: 71926.211, 71919.156, 7.056\n",
      "Epoch[1] sample[135/219] Loss: 76760.070, 76755.188, 4.887\n",
      "Epoch[1] sample[136/219] Loss: 75345.492, 75341.055, 4.438\n",
      "Epoch[1] sample[137/219] Loss: 72450.688, 72444.945, 5.745\n",
      "Epoch[1] sample[138/219] Loss: 73975.086, 73969.672, 5.413\n",
      "Epoch[1] sample[139/219] Loss: 74024.297, 74018.531, 5.765\n",
      "Epoch[1] sample[140/219] Loss: 73858.367, 73853.000, 5.369\n",
      "Epoch[1] sample[141/219] Loss: 75372.695, 75368.359, 4.334\n",
      "Epoch[1] sample[142/219] Loss: 74104.148, 74099.453, 4.697\n",
      "Epoch[1] sample[143/219] Loss: 73801.445, 73795.398, 6.049\n",
      "Epoch[1] sample[144/219] Loss: 73780.734, 73775.500, 5.231\n",
      "Epoch[1] sample[145/219] Loss: 75065.680, 75061.297, 4.384\n",
      "Epoch[1] sample[146/219] Loss: 77035.125, 77031.188, 3.934\n",
      "Epoch[1] sample[147/219] Loss: 73468.289, 73462.961, 5.329\n",
      "Epoch[1] sample[148/219] Loss: 70973.969, 70967.609, 6.361\n",
      "Epoch[1] sample[149/219] Loss: 76257.336, 76252.750, 4.583\n",
      "Epoch[1] sample[150/219] Loss: 74415.758, 74410.750, 5.006\n",
      "Epoch[1] sample[151/219] Loss: 73074.758, 73067.484, 7.277\n",
      "Epoch[1] sample[152/219] Loss: 70188.555, 70181.391, 7.166\n",
      "Epoch[1] sample[153/219] Loss: 71916.070, 71909.523, 6.549\n",
      "Epoch[1] sample[154/219] Loss: 72120.086, 72113.516, 6.569\n",
      "Epoch[1] sample[155/219] Loss: 76394.312, 76388.773, 5.537\n",
      "Epoch[1] sample[156/219] Loss: 70981.719, 70973.984, 7.732\n",
      "Epoch[1] sample[157/219] Loss: 71416.430, 71409.844, 6.587\n",
      "Epoch[1] sample[158/219] Loss: 72391.250, 72385.102, 6.149\n",
      "Epoch[1] sample[159/219] Loss: 75055.469, 75050.562, 4.905\n",
      "Epoch[1] sample[160/219] Loss: 76805.453, 76801.500, 3.955\n",
      "Epoch[1] sample[161/219] Loss: 73059.141, 73053.438, 5.702\n",
      "Epoch[1] sample[162/219] Loss: 75836.391, 75831.820, 4.568\n",
      "Epoch[1] sample[163/219] Loss: 74906.289, 74901.648, 4.642\n",
      "Epoch[1] sample[164/219] Loss: 72629.852, 72624.055, 5.797\n",
      "Epoch[1] sample[165/219] Loss: 70209.500, 70201.250, 8.254\n",
      "Epoch[1] sample[166/219] Loss: 74938.438, 74934.070, 4.367\n",
      "Epoch[1] sample[167/219] Loss: 73485.547, 73479.383, 6.163\n",
      "Epoch[1] sample[168/219] Loss: 72564.539, 72558.320, 6.216\n",
      "Epoch[1] sample[169/219] Loss: 75574.008, 75569.727, 4.279\n",
      "Epoch[1] sample[170/219] Loss: 72398.086, 72392.727, 5.356\n",
      "Epoch[1] sample[171/219] Loss: 71490.852, 71484.695, 6.153\n",
      "Epoch[1] sample[172/219] Loss: 73520.547, 73515.188, 5.360\n",
      "Epoch[1] sample[173/219] Loss: 74883.242, 74879.242, 4.000\n",
      "Epoch[1] sample[174/219] Loss: 73942.234, 73937.859, 4.372\n",
      "Epoch[1] sample[175/219] Loss: 74344.188, 74339.633, 4.557\n",
      "Epoch[1] sample[176/219] Loss: 73107.875, 73100.930, 6.943\n",
      "Epoch[1] sample[177/219] Loss: 73693.305, 73688.055, 5.250\n",
      "Epoch[1] sample[178/219] Loss: 73837.219, 73831.938, 5.281\n",
      "Epoch[1] sample[179/219] Loss: 74983.172, 74978.719, 4.455\n",
      "Epoch[1] sample[180/219] Loss: 75850.633, 75846.367, 4.267\n",
      "Epoch[1] sample[181/219] Loss: 73418.164, 73413.125, 5.042\n",
      "Epoch[1] sample[182/219] Loss: 75069.492, 75064.539, 4.956\n",
      "Epoch[1] sample[183/219] Loss: 72165.375, 72159.109, 6.268\n",
      "Epoch[1] sample[184/219] Loss: 73282.570, 73276.930, 5.641\n",
      "Epoch[1] sample[185/219] Loss: 74858.445, 74853.875, 4.568\n",
      "Epoch[1] sample[186/219] Loss: 71277.531, 71270.562, 6.972\n",
      "Epoch[1] sample[187/219] Loss: 73135.961, 73130.914, 5.047\n",
      "Epoch[1] sample[188/219] Loss: 72217.148, 72211.102, 6.049\n",
      "Epoch[1] sample[189/219] Loss: 73343.781, 73338.039, 5.746\n",
      "Epoch[1] sample[190/219] Loss: 70843.500, 70837.234, 6.265\n",
      "Epoch[1] sample[191/219] Loss: 72894.172, 72887.281, 6.888\n",
      "Epoch[1] sample[192/219] Loss: 72587.766, 72581.859, 5.909\n",
      "Epoch[1] sample[193/219] Loss: 73759.578, 73753.453, 6.127\n",
      "Epoch[1] sample[194/219] Loss: 75796.328, 75791.938, 4.387\n",
      "Epoch[1] sample[195/219] Loss: 70343.297, 70335.812, 7.485\n",
      "Epoch[1] sample[196/219] Loss: 72625.867, 72620.109, 5.757\n",
      "Epoch[1] sample[197/219] Loss: 75287.656, 75281.703, 5.954\n",
      "Epoch[1] sample[198/219] Loss: 68426.617, 68417.594, 9.022\n",
      "Epoch[1] sample[199/219] Loss: 74883.000, 74878.211, 4.787\n",
      "Epoch[1] sample[200/219] Loss: 74559.875, 74555.031, 4.842\n",
      "Epoch[1] sample[201/219] Loss: 71559.453, 71552.859, 6.596\n",
      "Epoch[1] sample[202/219] Loss: 73156.047, 73148.938, 7.109\n",
      "Epoch[1] sample[203/219] Loss: 71605.016, 71598.453, 6.564\n",
      "Epoch[1] sample[204/219] Loss: 75485.938, 75481.094, 4.846\n",
      "Epoch[1] sample[205/219] Loss: 74911.578, 74905.875, 5.706\n",
      "Epoch[1] sample[206/219] Loss: 71728.695, 71722.188, 6.506\n",
      "Epoch[1] sample[207/219] Loss: 74159.242, 74154.219, 5.022\n",
      "Epoch[1] sample[208/219] Loss: 73206.414, 73200.555, 5.856\n",
      "Epoch[1] sample[209/219] Loss: 74940.188, 74934.625, 5.562\n",
      "Epoch[1] sample[210/219] Loss: 71744.500, 71738.211, 6.291\n",
      "Epoch[1] sample[211/219] Loss: 72149.805, 72144.148, 5.658\n",
      "Epoch[1] sample[212/219] Loss: 71728.492, 71721.828, 6.660\n",
      "Epoch[1] sample[213/219] Loss: 75524.438, 75519.312, 5.125\n",
      "Epoch[1] sample[214/219] Loss: 72781.531, 72775.406, 6.123\n",
      "Epoch[1] sample[215/219] Loss: 74076.172, 74071.258, 4.913\n",
      "Epoch[1] sample[216/219] Loss: 71801.117, 71794.906, 6.212\n",
      "Epoch[1] sample[217/219] Loss: 72895.688, 72889.250, 6.434\n",
      "Epoch[1] sample[218/219] Loss: 56855.734, 56850.969, 4.767\n",
      "Epoch[2] sample[0/219] Loss: 68259.953, 68252.109, 7.846\n",
      "Epoch[2] sample[1/219] Loss: 73772.648, 73767.562, 5.085\n",
      "Epoch[2] sample[2/219] Loss: 75390.203, 75385.523, 4.677\n",
      "Epoch[2] sample[3/219] Loss: 74587.344, 74582.273, 5.067\n",
      "Epoch[2] sample[4/219] Loss: 72944.172, 72938.484, 5.685\n",
      "Epoch[2] sample[5/219] Loss: 75018.078, 75013.102, 4.975\n",
      "Epoch[2] sample[6/219] Loss: 73856.672, 73851.500, 5.169\n",
      "Epoch[2] sample[7/219] Loss: 73550.844, 73545.359, 5.487\n",
      "Epoch[2] sample[8/219] Loss: 72692.398, 72685.977, 6.422\n",
      "Epoch[2] sample[9/219] Loss: 74574.242, 74568.891, 5.350\n",
      "Epoch[2] sample[10/219] Loss: 75208.555, 75204.039, 4.519\n",
      "Epoch[2] sample[11/219] Loss: 73496.891, 73491.578, 5.314\n",
      "Epoch[2] sample[12/219] Loss: 69237.281, 69229.391, 7.892\n",
      "Epoch[2] sample[13/219] Loss: 75781.094, 75776.406, 4.688\n",
      "Epoch[2] sample[14/219] Loss: 72700.086, 72693.148, 6.939\n",
      "Epoch[2] sample[15/219] Loss: 75995.312, 75990.219, 5.092\n",
      "Epoch[2] sample[16/219] Loss: 75450.125, 75445.594, 4.532\n",
      "Epoch[2] sample[17/219] Loss: 72215.984, 72210.055, 5.927\n",
      "Epoch[2] sample[18/219] Loss: 75038.805, 75034.141, 4.667\n",
      "Epoch[2] sample[19/219] Loss: 75544.695, 75539.969, 4.723\n",
      "Epoch[2] sample[20/219] Loss: 72559.641, 72553.594, 6.050\n",
      "Epoch[2] sample[21/219] Loss: 72537.367, 72531.305, 6.064\n",
      "Epoch[2] sample[22/219] Loss: 75237.711, 75232.844, 4.865\n",
      "Epoch[2] sample[23/219] Loss: 74387.062, 74382.117, 4.949\n",
      "Epoch[2] sample[24/219] Loss: 75412.422, 75407.578, 4.847\n",
      "Epoch[2] sample[25/219] Loss: 70177.359, 70169.781, 7.582\n",
      "Epoch[2] sample[26/219] Loss: 73596.000, 73590.234, 5.764\n",
      "Epoch[2] sample[27/219] Loss: 74151.289, 74146.516, 4.776\n",
      "Epoch[2] sample[28/219] Loss: 74297.625, 74292.359, 5.264\n",
      "Epoch[2] sample[29/219] Loss: 76157.828, 76153.469, 4.359\n",
      "Epoch[2] sample[30/219] Loss: 76154.203, 76149.734, 4.472\n",
      "Epoch[2] sample[31/219] Loss: 73932.320, 73927.328, 4.993\n",
      "Epoch[2] sample[32/219] Loss: 71606.672, 71600.242, 6.428\n",
      "Epoch[2] sample[33/219] Loss: 75942.727, 75937.750, 4.977\n",
      "Epoch[2] sample[34/219] Loss: 74086.742, 74081.234, 5.508\n",
      "Epoch[2] sample[35/219] Loss: 75213.336, 75208.750, 4.587\n",
      "Epoch[2] sample[36/219] Loss: 72337.703, 72329.828, 7.876\n",
      "Epoch[2] sample[37/219] Loss: 73959.633, 73954.430, 5.203\n",
      "Epoch[2] sample[38/219] Loss: 73402.312, 73396.773, 5.540\n",
      "Epoch[2] sample[39/219] Loss: 76233.539, 76229.227, 4.310\n",
      "Epoch[2] sample[40/219] Loss: 74191.328, 74186.422, 4.905\n",
      "Epoch[2] sample[41/219] Loss: 74638.672, 74633.297, 5.377\n",
      "Epoch[2] sample[42/219] Loss: 75323.820, 75319.172, 4.650\n",
      "Epoch[2] sample[43/219] Loss: 75436.234, 75431.281, 4.956\n",
      "Epoch[2] sample[44/219] Loss: 74302.711, 74297.500, 5.212\n",
      "Epoch[2] sample[45/219] Loss: 72609.398, 72604.156, 5.244\n",
      "Epoch[2] sample[46/219] Loss: 72983.695, 72978.445, 5.250\n",
      "Epoch[2] sample[47/219] Loss: 72205.391, 72199.352, 6.036\n",
      "Epoch[2] sample[48/219] Loss: 75406.969, 75402.227, 4.741\n",
      "Epoch[2] sample[49/219] Loss: 73136.383, 73131.203, 5.181\n",
      "Epoch[2] sample[50/219] Loss: 72891.086, 72885.969, 5.117\n",
      "Epoch[2] sample[51/219] Loss: 71040.453, 71033.438, 7.017\n",
      "Epoch[2] sample[52/219] Loss: 70512.031, 70504.375, 7.657\n",
      "Epoch[2] sample[53/219] Loss: 75030.500, 75025.664, 4.837\n",
      "Epoch[2] sample[54/219] Loss: 69305.727, 69299.172, 6.553\n",
      "Epoch[2] sample[55/219] Loss: 74691.344, 74685.547, 5.797\n",
      "Epoch[2] sample[56/219] Loss: 72649.203, 72643.289, 5.910\n",
      "Epoch[2] sample[57/219] Loss: 76490.227, 76486.062, 4.164\n",
      "Epoch[2] sample[58/219] Loss: 70766.203, 70759.164, 7.042\n",
      "Epoch[2] sample[59/219] Loss: 75331.586, 75326.844, 4.740\n",
      "Epoch[2] sample[60/219] Loss: 73053.406, 73047.836, 5.571\n",
      "Epoch[2] sample[61/219] Loss: 70656.633, 70650.266, 6.365\n",
      "Epoch[2] sample[62/219] Loss: 73342.570, 73337.188, 5.382\n",
      "Epoch[2] sample[63/219] Loss: 77189.148, 77185.008, 4.140\n",
      "Epoch[2] sample[64/219] Loss: 75392.578, 75387.414, 5.165\n",
      "Epoch[2] sample[65/219] Loss: 71860.688, 71854.688, 6.000\n",
      "Epoch[2] sample[66/219] Loss: 72413.422, 72407.703, 5.721\n",
      "Epoch[2] sample[67/219] Loss: 73892.695, 73886.609, 6.087\n",
      "Epoch[2] sample[68/219] Loss: 72619.914, 72613.609, 6.301\n",
      "Epoch[2] sample[69/219] Loss: 73249.156, 73243.109, 6.048\n",
      "Epoch[2] sample[70/219] Loss: 74039.930, 74034.828, 5.100\n",
      "Epoch[2] sample[71/219] Loss: 74789.820, 74784.562, 5.257\n",
      "Epoch[2] sample[72/219] Loss: 73555.883, 73550.734, 5.152\n",
      "Epoch[2] sample[73/219] Loss: 73596.469, 73590.930, 5.540\n",
      "Epoch[2] sample[74/219] Loss: 75278.531, 75273.953, 4.580\n",
      "Epoch[2] sample[75/219] Loss: 75055.820, 75050.672, 5.151\n",
      "Epoch[2] sample[76/219] Loss: 72269.094, 72263.188, 5.904\n",
      "Epoch[2] sample[77/219] Loss: 72361.594, 72355.289, 6.302\n",
      "Epoch[2] sample[78/219] Loss: 70686.383, 70680.438, 5.945\n",
      "Epoch[2] sample[79/219] Loss: 74580.484, 74574.984, 5.502\n",
      "Epoch[2] sample[80/219] Loss: 73368.758, 73362.562, 6.196\n",
      "Epoch[2] sample[81/219] Loss: 69912.336, 69904.938, 7.398\n",
      "Epoch[2] sample[82/219] Loss: 70490.578, 70483.688, 6.887\n",
      "Epoch[2] sample[83/219] Loss: 74002.242, 73996.953, 5.290\n",
      "Epoch[2] sample[84/219] Loss: 75029.133, 75024.453, 4.676\n",
      "Epoch[2] sample[85/219] Loss: 74076.766, 74071.352, 5.416\n",
      "Epoch[2] sample[86/219] Loss: 72718.508, 72712.266, 6.243\n",
      "Epoch[2] sample[87/219] Loss: 71218.766, 71212.172, 6.597\n",
      "Epoch[2] sample[88/219] Loss: 72971.664, 72965.703, 5.960\n",
      "Epoch[2] sample[89/219] Loss: 75880.438, 75875.531, 4.910\n",
      "Epoch[2] sample[90/219] Loss: 71567.656, 71561.453, 6.206\n",
      "Epoch[2] sample[91/219] Loss: 74326.789, 74321.383, 5.405\n",
      "Epoch[2] sample[92/219] Loss: 74944.203, 74939.367, 4.833\n",
      "Epoch[2] sample[93/219] Loss: 71619.531, 71613.023, 6.509\n",
      "Epoch[2] sample[94/219] Loss: 76846.586, 76842.281, 4.302\n",
      "Epoch[2] sample[95/219] Loss: 71506.102, 71499.875, 6.223\n",
      "Epoch[2] sample[96/219] Loss: 72887.648, 72881.852, 5.801\n",
      "Epoch[2] sample[97/219] Loss: 73802.328, 73796.727, 5.598\n",
      "Epoch[2] sample[98/219] Loss: 75497.727, 75492.969, 4.756\n",
      "Epoch[2] sample[99/219] Loss: 73950.273, 73944.344, 5.933\n",
      "Epoch[2] sample[100/219] Loss: 73809.062, 73803.266, 5.800\n",
      "Epoch[2] sample[101/219] Loss: 74651.531, 74646.625, 4.910\n",
      "Epoch[2] sample[102/219] Loss: 74471.961, 74466.703, 5.256\n",
      "Epoch[2] sample[103/219] Loss: 73655.891, 73650.469, 5.420\n",
      "Epoch[2] sample[104/219] Loss: 72049.898, 72043.656, 6.244\n",
      "Epoch[2] sample[105/219] Loss: 72375.977, 72369.320, 6.654\n",
      "Epoch[2] sample[106/219] Loss: 73856.023, 73850.492, 5.535\n",
      "Epoch[2] sample[107/219] Loss: 71770.688, 71764.305, 6.382\n",
      "Epoch[2] sample[108/219] Loss: 72592.273, 72586.586, 5.691\n",
      "Epoch[2] sample[109/219] Loss: 72298.781, 72292.938, 5.845\n",
      "Epoch[2] sample[110/219] Loss: 71951.070, 71944.742, 6.330\n",
      "Epoch[2] sample[111/219] Loss: 72817.312, 72811.164, 6.146\n",
      "Epoch[2] sample[112/219] Loss: 73433.375, 73428.281, 5.097\n",
      "Epoch[2] sample[113/219] Loss: 70293.852, 70287.547, 6.302\n",
      "Epoch[2] sample[114/219] Loss: 73324.672, 73318.703, 5.968\n",
      "Epoch[2] sample[115/219] Loss: 75934.625, 75930.281, 4.346\n",
      "Epoch[2] sample[116/219] Loss: 75647.969, 75643.359, 4.611\n",
      "Epoch[2] sample[117/219] Loss: 69456.984, 69449.016, 7.966\n",
      "Epoch[2] sample[118/219] Loss: 75001.805, 74996.547, 5.259\n",
      "Epoch[2] sample[119/219] Loss: 70811.727, 70805.625, 6.102\n",
      "Epoch[2] sample[120/219] Loss: 74256.180, 74250.664, 5.516\n",
      "Epoch[2] sample[121/219] Loss: 71655.805, 71649.031, 6.775\n",
      "Epoch[2] sample[122/219] Loss: 73287.023, 73281.359, 5.662\n",
      "Epoch[2] sample[123/219] Loss: 73607.039, 73601.969, 5.071\n",
      "Epoch[2] sample[124/219] Loss: 74653.555, 74648.922, 4.630\n",
      "Epoch[2] sample[125/219] Loss: 72599.297, 72593.625, 5.669\n",
      "Epoch[2] sample[126/219] Loss: 72248.094, 72242.281, 5.813\n",
      "Epoch[2] sample[127/219] Loss: 74771.234, 74765.516, 5.717\n",
      "Epoch[2] sample[128/219] Loss: 71060.117, 71053.156, 6.958\n",
      "Epoch[2] sample[129/219] Loss: 72759.656, 72754.031, 5.623\n",
      "Epoch[2] sample[130/219] Loss: 69724.672, 69717.047, 7.628\n",
      "Epoch[2] sample[131/219] Loss: 72302.969, 72297.188, 5.783\n",
      "Epoch[2] sample[132/219] Loss: 71818.648, 71812.703, 5.943\n",
      "Epoch[2] sample[133/219] Loss: 75530.945, 75526.258, 4.686\n",
      "Epoch[2] sample[134/219] Loss: 74094.117, 74089.000, 5.118\n",
      "Epoch[2] sample[135/219] Loss: 72263.031, 72257.023, 6.008\n",
      "Epoch[2] sample[136/219] Loss: 74381.195, 74376.039, 5.159\n",
      "Epoch[2] sample[137/219] Loss: 74918.969, 74914.039, 4.932\n",
      "Epoch[2] sample[138/219] Loss: 75288.344, 75283.766, 4.582\n",
      "Epoch[2] sample[139/219] Loss: 74885.023, 74880.078, 4.946\n",
      "Epoch[2] sample[140/219] Loss: 73966.836, 73961.141, 5.696\n",
      "Epoch[2] sample[141/219] Loss: 73009.445, 73003.922, 5.527\n",
      "Epoch[2] sample[142/219] Loss: 74335.602, 74330.508, 5.091\n",
      "Epoch[2] sample[143/219] Loss: 74520.000, 74514.484, 5.513\n",
      "Epoch[2] sample[144/219] Loss: 77150.578, 77146.562, 4.015\n",
      "Epoch[2] sample[145/219] Loss: 73652.469, 73647.188, 5.282\n",
      "Epoch[2] sample[146/219] Loss: 75626.109, 75621.852, 4.256\n",
      "Epoch[2] sample[147/219] Loss: 73732.375, 73727.562, 4.811\n",
      "Epoch[2] sample[148/219] Loss: 73222.500, 73216.750, 5.752\n",
      "Epoch[2] sample[149/219] Loss: 73317.219, 73311.406, 5.816\n",
      "Epoch[2] sample[150/219] Loss: 76380.609, 76376.438, 4.174\n",
      "Epoch[2] sample[151/219] Loss: 74912.633, 74907.555, 5.077\n",
      "Epoch[2] sample[152/219] Loss: 75838.977, 75834.516, 4.464\n",
      "Epoch[2] sample[153/219] Loss: 72867.148, 72860.781, 6.368\n",
      "Epoch[2] sample[154/219] Loss: 73823.453, 73818.359, 5.090\n",
      "Epoch[2] sample[155/219] Loss: 71757.430, 71751.430, 5.997\n",
      "Epoch[2] sample[156/219] Loss: 74093.414, 74088.398, 5.013\n",
      "Epoch[2] sample[157/219] Loss: 73386.016, 73380.742, 5.270\n",
      "Epoch[2] sample[158/219] Loss: 73144.172, 73139.047, 5.123\n",
      "Epoch[2] sample[159/219] Loss: 74633.102, 74627.867, 5.234\n",
      "Epoch[2] sample[160/219] Loss: 72888.922, 72882.766, 6.153\n",
      "Epoch[2] sample[161/219] Loss: 77932.469, 77928.641, 3.828\n",
      "Epoch[2] sample[162/219] Loss: 73718.477, 73712.938, 5.536\n",
      "Epoch[2] sample[163/219] Loss: 74438.672, 74433.898, 4.773\n",
      "Epoch[2] sample[164/219] Loss: 71733.258, 71726.688, 6.572\n",
      "Epoch[2] sample[165/219] Loss: 73106.398, 73100.773, 5.626\n",
      "Epoch[2] sample[166/219] Loss: 74685.742, 74680.750, 4.996\n",
      "Epoch[2] sample[167/219] Loss: 72270.688, 72264.695, 5.989\n",
      "Epoch[2] sample[168/219] Loss: 72540.914, 72534.742, 6.175\n",
      "Epoch[2] sample[169/219] Loss: 73006.672, 73000.562, 6.108\n",
      "Epoch[2] sample[170/219] Loss: 73063.250, 73057.078, 6.175\n",
      "Epoch[2] sample[171/219] Loss: 71947.703, 71941.914, 5.791\n",
      "Epoch[2] sample[172/219] Loss: 69029.336, 69021.523, 7.812\n",
      "Epoch[2] sample[173/219] Loss: 75698.914, 75693.984, 4.932\n",
      "Epoch[2] sample[174/219] Loss: 74443.133, 74438.250, 4.883\n",
      "Epoch[2] sample[175/219] Loss: 75377.586, 75372.906, 4.679\n",
      "Epoch[2] sample[176/219] Loss: 74089.797, 74084.383, 5.411\n",
      "Epoch[2] sample[177/219] Loss: 74518.602, 74513.359, 5.242\n",
      "Epoch[2] sample[178/219] Loss: 73532.852, 73527.641, 5.209\n",
      "Epoch[2] sample[179/219] Loss: 72422.344, 72416.836, 5.507\n",
      "Epoch[2] sample[180/219] Loss: 73281.117, 73274.914, 6.201\n",
      "Epoch[2] sample[181/219] Loss: 71396.492, 71390.039, 6.454\n",
      "Epoch[2] sample[182/219] Loss: 71427.188, 71420.711, 6.474\n",
      "Epoch[2] sample[183/219] Loss: 76044.938, 76040.344, 4.596\n",
      "Epoch[2] sample[184/219] Loss: 72582.922, 72576.398, 6.524\n",
      "Epoch[2] sample[185/219] Loss: 73816.250, 73811.141, 5.110\n",
      "Epoch[2] sample[186/219] Loss: 71456.797, 71450.516, 6.278\n",
      "Epoch[2] sample[187/219] Loss: 73225.992, 73219.914, 6.074\n",
      "Epoch[2] sample[188/219] Loss: 75286.180, 75281.234, 4.943\n",
      "Epoch[2] sample[189/219] Loss: 74659.703, 74654.000, 5.699\n",
      "Epoch[2] sample[190/219] Loss: 72961.336, 72955.125, 6.207\n",
      "Epoch[2] sample[191/219] Loss: 73581.258, 73574.859, 6.402\n",
      "Epoch[2] sample[192/219] Loss: 76058.125, 76053.633, 4.494\n",
      "Epoch[2] sample[193/219] Loss: 76077.484, 76072.805, 4.678\n",
      "Epoch[2] sample[194/219] Loss: 71785.688, 71779.281, 6.408\n",
      "Epoch[2] sample[195/219] Loss: 75010.539, 75005.617, 4.918\n",
      "Epoch[2] sample[196/219] Loss: 72538.703, 72533.391, 5.315\n",
      "Epoch[2] sample[197/219] Loss: 75250.156, 75245.500, 4.653\n",
      "Epoch[2] sample[198/219] Loss: 73563.461, 73557.766, 5.694\n",
      "Epoch[2] sample[199/219] Loss: 72974.742, 72969.336, 5.404\n",
      "Epoch[2] sample[200/219] Loss: 73826.305, 73821.367, 4.936\n",
      "Epoch[2] sample[201/219] Loss: 74020.953, 74015.523, 5.427\n",
      "Epoch[2] sample[202/219] Loss: 72509.500, 72503.719, 5.781\n",
      "Epoch[2] sample[203/219] Loss: 74226.875, 74221.422, 5.449\n",
      "Epoch[2] sample[204/219] Loss: 74409.141, 74403.953, 5.190\n",
      "Epoch[2] sample[205/219] Loss: 75408.953, 75404.297, 4.660\n",
      "Epoch[2] sample[206/219] Loss: 69190.406, 69182.828, 7.576\n",
      "Epoch[2] sample[207/219] Loss: 74248.086, 74243.062, 5.026\n",
      "Epoch[2] sample[208/219] Loss: 71581.695, 71575.188, 6.505\n",
      "Epoch[2] sample[209/219] Loss: 70822.242, 70815.719, 6.526\n",
      "Epoch[2] sample[210/219] Loss: 75125.141, 75120.023, 5.119\n",
      "Epoch[2] sample[211/219] Loss: 72071.547, 72065.359, 6.185\n",
      "Epoch[2] sample[212/219] Loss: 72012.570, 72006.719, 5.851\n",
      "Epoch[2] sample[213/219] Loss: 74957.352, 74952.547, 4.806\n",
      "Epoch[2] sample[214/219] Loss: 74402.961, 74397.562, 5.400\n",
      "Epoch[2] sample[215/219] Loss: 73232.219, 73226.000, 6.216\n",
      "Epoch[2] sample[216/219] Loss: 71811.250, 71804.422, 6.830\n",
      "Epoch[2] sample[217/219] Loss: 71773.867, 71767.273, 6.592\n",
      "Epoch[2] sample[218/219] Loss: 56560.754, 56555.938, 4.815\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "    async function download(id, filename, size) {\n",
       "      if (!google.colab.kernel.accessAllowed) {\n",
       "        return;\n",
       "      }\n",
       "      const div = document.createElement('div');\n",
       "      const label = document.createElement('label');\n",
       "      label.textContent = `Downloading \"${filename}\": `;\n",
       "      div.appendChild(label);\n",
       "      const progress = document.createElement('progress');\n",
       "      progress.max = size;\n",
       "      div.appendChild(progress);\n",
       "      document.body.appendChild(div);\n",
       "\n",
       "      const buffers = [];\n",
       "      let downloaded = 0;\n",
       "\n",
       "      const channel = await google.colab.kernel.comms.open(id);\n",
       "      // Send a message to notify the kernel that we're ready.\n",
       "      channel.send({})\n",
       "\n",
       "      for await (const message of channel.messages) {\n",
       "        // Send a message to notify the kernel that we're ready.\n",
       "        channel.send({})\n",
       "        if (message.buffers) {\n",
       "          for (const buffer of message.buffers) {\n",
       "            buffers.push(buffer);\n",
       "            downloaded += buffer.byteLength;\n",
       "            progress.value = downloaded;\n",
       "          }\n",
       "        }\n",
       "      }\n",
       "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
       "      const a = document.createElement('a');\n",
       "      a.href = window.URL.createObjectURL(blob);\n",
       "      a.download = filename;\n",
       "      div.appendChild(a);\n",
       "      a.click();\n",
       "      div.remove();\n",
       "    }\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "download(\"download_17e78eac-0bbd-4261-b28f-4c45ed0a2efc\", \"vae.torch\", 17386563)"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "epochs = 3\n",
    "model.training = True\n",
    "\n",
    "for epoch in range(epochs): \n",
    "  for idx, (images, _) in enumerate(dataloader): \n",
    "    images = images.to(device)\n",
    "    y, mu, log_var = model(images)\n",
    "    loss, bce, kld = loss_fn(y, images, mu, log_var)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f\"Epoch[{epoch}] sample[{idx}/{len(dataloader)}] Loss: {loss:.3f}, {bce:.3f}, {kld:.3f}\")\n",
    "\n",
    "torch.save(model.state_dict(), 'vae.torch')\n",
    "\n",
    "# auto download after train\n",
    "from google.colab import files\n",
    "files.download('vae.torch')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
