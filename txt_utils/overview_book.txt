CAP 4: DynamicProgramming: 
	How extract policy ?

	recap V_pi (s) = EXP(pi) [ G_t | S_t = s]
	
	Iterative policy evaluation: ALG FOR ESTIMATE VALUE OF EACH STATE
		based on fixed point, 
		V_t+1(s) = EXPECT(pi) [ R_t+1 + gamma*V_k(S_t+1) | S_t=s]
	
	Policy imporvement: 
		Q_funct ---> test what's happening if we try a particular action (a) 
		Q_pi(s,a) = EXPECT(pi) [ R_t+1 + gamma*V_pi(S_t+1) | s_t=s, A_t=a]
			
		Notice the difference in the value of V_x ---> k vs pi
		using K we are looking for a value for each state
		using pi we meaning that we use the classical policy			
		
		CONSIDERING ALL THE ACTION =  Greedy Version
			select pi'(s) = argmax (a) Q_pi(s, a)
			use any pi that has Q > V |<--- if using a different action the value increase, 
							it's beacuse the new vers is better that the old one
	
	
	Policy iteration 
		[ðœ‹0 --|EVAL|--> V_ðœ‹0 --|IMPROV|-->] [ðœ‹1 --|EVAL|-->V_ðœ‹1] .... --|IMPROV|-->ðœ‹* --|EVAL|--> V_ðœ‹*
		FOR EACH RUN 2 PHASE ^
	
	Value iteration
		Stop the policy iteration for optimization 
		just include bellman operator --> max(a)EXPECT [R_t+1 +  gamma*V_k(s_t+1) | S_t = s, A_t = a]
		
