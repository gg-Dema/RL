{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "import sys\n",
    "sys.path.append('/home/dema/Sapienza/primo_2022/RL/venv/lib/python3.10/site-packages')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "Aq405erfpGKv"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dema/Sapienza/primo_2022/RL/venv/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: libtorch_cuda_cu.so: cannot open shared object file: No such file or directory\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "import pickle\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "raB0pq_vuaak",
    "outputId": "482c0c4c-96e0-4625-f4c7-285501aacc0c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x5Y4Y3eXugTc",
    "outputId": "9b85a79b-7268-4188-ee10-3c542457a52f"
   },
   "outputs": [],
   "source": [
    "env_id = \"CarRacing-v2\"\n",
    "\n",
    "# Create the env\n",
    "env = gym.make(env_id, continuous=False, domain_randomize=False)\n",
    "\n",
    "# Create the evaluation env\n",
    "eval_env = gym.make(env_id, continuous=False, domain_randomize=False)\n",
    "\n",
    "# Get the state space and action space\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "n_frames = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "NpSZGAuKulI6"
   },
   "outputs": [],
   "source": [
    "from policy import Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "5kAeq1Rl1Hyj"
   },
   "outputs": [],
   "source": [
    "MAX_PATIENCE = 100 # Maximum consecutive steps with negative reward "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "sYk1se-R3vmh"
   },
   "outputs": [],
   "source": [
    "def evaluate_agent(env, n_eval_episodes, policy):\n",
    "    episode_rewards = []\n",
    "    \n",
    "    for episode in range(n_eval_episodes):\n",
    "        state = env.reset() # state reset\n",
    "        \n",
    "        # perform noop for 60 steps (noisy start)\n",
    "        for i in range(60):\n",
    "            state,_,_,_,_ = env.step(0)\n",
    "            \n",
    "        \n",
    "        done = False\n",
    "        \n",
    "        # stats\n",
    "        total_rewards_ep = 0\n",
    "        negative_reward_patience = MAX_PATIENCE\n",
    "        \n",
    "        # state\n",
    "        states = deque(maxlen=4)\n",
    "        for i in range(n_frames):\n",
    "            states.append(state)\n",
    "            \n",
    "        while not done:\n",
    "            # perform action\n",
    "            action, _ = policy.act(states, exploration=False)\n",
    "            \n",
    "            state, reward, done, info, _ = env.step(action)\n",
    "            states.append(state)\n",
    "            \n",
    "            # handle patience\n",
    "            if reward >=0:\n",
    "                negative_reward_patience = MAX_PATIENCE\n",
    "            else:\n",
    "                negative_reward_patience -= 1\n",
    "                if negative_reward_patience == 0:\n",
    "                    done = True\n",
    "            if done: reward = -100\n",
    "                    \n",
    "            # stats\n",
    "            total_rewards_ep += reward\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        # stats\n",
    "        episode_rewards.append(total_rewards_ep)\n",
    "        \n",
    "    # stats\n",
    "    mean_reward = np.mean(episode_rewards)\n",
    "    std_reward = np.std(episode_rewards)\n",
    "\n",
    "    return mean_reward, std_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Uzb4bInRxMsx"
   },
   "outputs": [],
   "source": [
    "def reinforce(policy, optimizer, n_training_episodes=1000, gamma=0.99, print_every=5):\n",
    "    # stats\n",
    "    scores_deque = deque(maxlen=100)\n",
    "    \n",
    "    for i_episode in range(1, n_training_episodes+1):\n",
    "        saved_log_probs = [] # stores log probs during episode\n",
    "        rewards = [] # stores rewards during episode\n",
    "        \n",
    "        # init episode\n",
    "        state = env.reset()\n",
    "        for i in range(60):\n",
    "            state,_,_,_,_ = env.step(0)\n",
    "        done = False\n",
    "        \n",
    "        negative_reward_patience = MAX_PATIENCE\n",
    "        states = deque(maxlen=4)\n",
    "        for i in range(n_frames):\n",
    "            states.append(state)\n",
    "            \n",
    "            \n",
    "        while not done:\n",
    "            action, log_prob = policy.act(states)\n",
    "            \n",
    "            saved_log_probs.append(log_prob)\n",
    "            \n",
    "            state, reward, done, _, _ = env.step(action)\n",
    "            \n",
    "            states.append(state)\n",
    "            \n",
    "            if reward >=0:\n",
    "                negative_reward_patience = MAX_PATIENCE\n",
    "            else:\n",
    "                negative_reward_patience -= 1\n",
    "                if negative_reward_patience == 0:\n",
    "                    done = True\n",
    "            if done: reward = -100\n",
    "                    \n",
    "            rewards.append(reward)\n",
    "            \n",
    "            if done:\n",
    "                break \n",
    "        scores_deque.append(sum(rewards))\n",
    "\n",
    "        \n",
    "        rewards = np.array(rewards)\n",
    "        discounts = np.power(gamma, np.arange(len(rewards)))\n",
    "        \n",
    "        policy_loss = 0\n",
    "        for t in range(len(rewards)):\n",
    "            G = (rewards[t:]*discounts[:len(rewards)-t]).sum()\n",
    "            policy_loss += saved_log_probs[t]*G*discounts[t]\n",
    "            \n",
    "        optimizer.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        optimizer.step() # contains theta + alpha\n",
    "        \n",
    "        if i_episode % print_every == 0:\n",
    "            print(f'''Episode {i_episode}\n",
    "                    \\tAverage Score: {np.mean(scores_deque)}\n",
    "                    \\tLast Score: {rewards.sum()}\n",
    "                    \\tEval Score: {evaluate_agent(eval_env,5,policy)}''')\n",
    "            torch.save(policy, 'model.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = Policy(n_frames, n_actions, 32).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "bQYqOdbiy0ez"
   },
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(policy.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "esS1pRU6D9CS",
    "outputId": "21b11365-29ed-45d3-add8-568ed969baaa"
   },
   "outputs": [],
   "source": [
    "reinforce(policy, optimizer)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
