{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73118f47",
   "metadata": {},
   "source": [
    "# Value iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c3d266",
   "metadata": {},
   "source": [
    "![](imgs/value_iteration_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac640f47",
   "metadata": {},
   "source": [
    "![](imgs/value_iteration_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86de16a6",
   "metadata": {},
   "source": [
    "![](imgs/value_iteration_3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "613d21ee",
   "metadata": {},
   "source": [
    "![](imgs/value_iteration_4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb1672c",
   "metadata": {},
   "source": [
    "![](imgs/value_iteration_5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24962d48",
   "metadata": {},
   "source": [
    "![](imgs/value_iteration_6.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2523810d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import gym\n",
    "import time\n",
    "from gym import spaces\n",
    "import os\n",
    "\n",
    "\n",
    "# custom 2d grid world enviroment\n",
    "class GridWorld(gym.Env):\n",
    "    metadata = {'render.modes': ['console']}\n",
    "\n",
    "    # actions available\n",
    "    UP = 0\n",
    "    LEFT = 1\n",
    "    DOWN = 2\n",
    "    RIGHT = 3\n",
    "\n",
    "\n",
    "    def __init__(self, width, height):\n",
    "        super(GridWorld, self).__init__()\n",
    "        self.ACTION_NAMES = [\"UP\", \"LEFT\", \"DOWN\", \"RIGHT\"]\n",
    "        self.num_actions = 4\n",
    "\n",
    "        self.size = width * height  # size of the grid world\n",
    "        self.num_states = self.size\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        self.num_obstacles = int((width+height)/2)\n",
    "        self.end_state = np.array([height - 1, width - 1], dtype=np.uint8) # goal state = bottom right cell\n",
    "\n",
    "        # actions of agents : up, down, left and right\n",
    "        self.action_space = spaces.Discrete(4)\n",
    "        # observation : cell indices in the grid\n",
    "        self.observation_space = spaces.MultiDiscrete([self.height, self.width])\n",
    "\n",
    "        self.obstacles = np.zeros((height, width))\n",
    "\n",
    "        for i in range(self.num_obstacles):\n",
    "            self.obstacles[ random.randrange(height) , random.randrange(width)] = 1\n",
    "\n",
    "        self.num_steps = 0\n",
    "        self.max_steps = height*width\n",
    "\n",
    "        self.current_state = np.zeros((2), np.uint8)#init state = [0,0]\n",
    "\n",
    "        self.directions = np.array([\n",
    "            [-1,0], #UP\n",
    "            [0,-1], #LEFT\n",
    "            [1,0], #DOWN\n",
    "            [0,1] #RIGHT\n",
    "        ])\n",
    "\n",
    "    def transition_function(self, s, a):\n",
    "        s_prime =  np.zeros((2), np.uint8)\n",
    "        #s_prime = ?????\n",
    "        s_prime = s + self.directions[a,:]\n",
    "\n",
    "        if s_prime[0] < self.height and s_prime[1] < self.width and (s_prime >= 0).all():\n",
    "            if self.obstacles[s_prime[0], s_prime[1]] == 0 :\n",
    "                return s_prime\n",
    "\n",
    "        return s\n",
    "\n",
    "    def transition_probabilities(self, s, a):\n",
    "        prob_next_state = np.zeros((self.heigth, self.width))\n",
    "        s_prime = self.transition_function(s, a)\n",
    "\n",
    "        prob_next_state[s_prime[0], s_prime[1]] = 1.0\n",
    "\n",
    "        return prob_next_state#.flatten()\n",
    "\n",
    "    def reward_function(self,s):\n",
    "        r = 0\n",
    "        #r= ????\n",
    "        if (s == self.end_state).all():\n",
    "            r = 1\n",
    "\n",
    "        return r\n",
    "\n",
    "    def termination_condition(self, s):\n",
    "        done = False\n",
    "        #done= ???\n",
    "\n",
    "        done = (s == self.end_state).all() or self.num_steps > self.max_steps\n",
    "\n",
    "        return done\n",
    "\n",
    "    def step(self, action):\n",
    "        s_prime = self.transition_function(self.current_state, action)\n",
    "        reward = self.reward_function(s_prime)\n",
    "        done = self.termination_condition(s_prime)\n",
    "\n",
    "        self.current_state = s_prime\n",
    "        self.num_steps += 1\n",
    "\n",
    "        return self.current_state, reward, done, None\n",
    "\n",
    "    def render(self):\n",
    "        '''\n",
    "            render the state\n",
    "        '''\n",
    "\n",
    "        row = self.current_state[0]\n",
    "        col = self.current_state[1]\n",
    "\n",
    "        for r in range(self.height):\n",
    "            for c in range(self.width):\n",
    "                if r == row and c == col:\n",
    "                    print(\"| A \", end='')\n",
    "                elif r == self.end_state[0] and c == self.end_state[1]:\n",
    "                    print(\"| G \", end='')\n",
    "                else:\n",
    "                    if self.obstacles[r,c] == 1:\n",
    "                        print('|///', end='')\n",
    "                    else:\n",
    "                        print('|___', end='')\n",
    "            print('|')\n",
    "        print('\\n')\n",
    "\n",
    "\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_state = np.zeros((2), np.uint8)\n",
    "\n",
    "        return self.current_state\n",
    "\n",
    "    def reward_probabilities(self):\n",
    "        rewards = np.zeros((self.num_states))\n",
    "        i = 0\n",
    "        for r in range(self.height):\n",
    "            for c in range(self.width):\n",
    "                state = np.array([r,c], dtype=np.uint8)\n",
    "                rewards[i] = self.reward_function(state)\n",
    "                i+=1\n",
    "\n",
    "        return rewards\n",
    "\n",
    "    def close(self):\n",
    "        pass\n",
    "    \n",
    "    \n",
    "class NonDeterministicGridWorld(GridWorld):\n",
    "    def __init__(self, width, height, p=0.8):\n",
    "        super(NonDeterministicGridWorld, self).__init__(width, height)\n",
    "        self.probability_right_action = p\n",
    "\n",
    "    def transition_function(self, s, a):\n",
    "        s_prime = s + self.directions[a, :]\n",
    "\n",
    "        #with probability 1 - p diagonal movement\n",
    "        if random.random() <= 1 - self.probability_right_action:\n",
    "            if random.random() < 0.5:\n",
    "                s_prime = s_prime + self.directions[(a+1)%self.num_actions, :]\n",
    "            else:\n",
    "                s_prime = s_prime + self.directions[(a-1)%self.num_actions, :]\n",
    "\n",
    "\n",
    "        if s_prime[0] < self.height and s_prime[1] < self.width and (s_prime >= 0).all():\n",
    "            if self.obstacles[s_prime[0], s_prime[1]] == 0 :\n",
    "                return s_prime\n",
    "\n",
    "        return s\n",
    "\n",
    "    def transition_probabilities(self, s, a):\n",
    "        cells = []\n",
    "        probs = []\n",
    "        prob_next_state = np.zeros((self.height, self.width))\n",
    "        s_prime_right =  s + self.directions[a, :]\n",
    "        if s_prime_right[0] < self.height and s_prime_right[1] < self.width and (s_prime_right >= 0).all():\n",
    "            if self.obstacles[s_prime_right[0], s_prime_right[1]] == 0 :\n",
    "                prob_next_state[s_prime_right[0], s_prime_right[1]] = self.probability_right_action\n",
    "                cells.append(s_prime_right)\n",
    "                probs.append(self.probability_right_action)\n",
    "\n",
    "        s_prime = s_prime_right + self.directions[(a + 1) % self.num_actions, :]\n",
    "        if s_prime[0] < self.height and s_prime[1] < self.width and (s_prime >= 0).all():\n",
    "            if self.obstacles[s_prime[0], s_prime[1]] == 0 :\n",
    "                prob_next_state[s_prime[0], s_prime[1]] = (1 - self.probability_right_action) / 2\n",
    "                cells.append(s_prime.copy())\n",
    "                probs.append((1 - self.probability_right_action) / 2)\n",
    "\n",
    "        s_prime = s_prime_right + self.directions[(a - 1) % self.num_actions, :]\n",
    "        if s_prime[0] < self.height and s_prime[1] < self.width and (s_prime >= 0).all():\n",
    "            if self.obstacles[s_prime[0], s_prime[1]] == 0 :\n",
    "                prob_next_state[s_prime[0], s_prime[1]] = (1 - self.probability_right_action) / 2\n",
    "                cells.append(s_prime.copy())\n",
    "                probs.append((1 - self.probability_right_action) / 2)\n",
    "\n",
    "        #normalization\n",
    "        sump = sum(probs)\n",
    "        #for cell in cells:\n",
    "        #    prob_next_state[cell[0], cell[1]] /= sump\n",
    "        prob_next_state[s[0], s[1]] = 1 - sump\n",
    "        return prob_next_state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "602977ce",
   "metadata": {},
   "source": [
    "To apply value iteration we need the **transition probabilities** and the **reward function**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01dddba2",
   "metadata": {},
   "source": [
    "Print the probability over the next state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "378bbbdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| A |___|///|\n",
      "|___|___|___|\n",
      "|___|///|///|\n",
      "|___|___|///|\n",
      "|___|___| G |\n",
      "\n",
      "\n",
      "[[0.1 0.  0. ]\n",
      " [0.8 0.1 0. ]\n",
      " [0.  0.  0. ]\n",
      " [0.  0.  0. ]\n",
      " [0.  0.  0. ]]\n"
     ]
    }
   ],
   "source": [
    "env = NonDeterministicGridWorld(3,5)\n",
    "state = env.reset()\n",
    "env.render()\n",
    "#next state if we start from state 0,0 and we do action down\n",
    "next_state_prob = env.transition_probabilities(state, 2)\n",
    "print(next_state_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd61c23d",
   "metadata": {},
   "source": [
    "reward values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9826985",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n"
     ]
    }
   ],
   "source": [
    "print(env.reward_probabilities())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c71dea30",
   "metadata": {},
   "source": [
    "# Value iteration algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff5512d",
   "metadata": {},
   "source": [
    "![](imgs/value_iteration.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "408efa64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(env, sigma=0.99):\n",
    "    # sigma = discount factor\n",
    "    #initialize values\n",
    "    values = np.zeros((env.num_states))\n",
    "    STATES = np.zeros((env.num_states, 2), dtype=np.uint8)\n",
    "    REWARDS = env.reward_probabilities()\n",
    "    i = 0\n",
    "    for r in range(env.height):\n",
    "        for c in range(env.width):\n",
    "            state = np.array([r, c], dtype=np.uint8)\n",
    "            STATES[i] = state\n",
    "            i += 1\n",
    "    delta = 1\n",
    "    while delta > 0.1:\n",
    "        v_old = values.copy()\n",
    "        for s in range(env.num_states):\n",
    "\n",
    "            state = STATES[s]\n",
    "            max_va = -1\n",
    "            for a in range(env.num_actions):\n",
    "                next_state_prob = env.transition_probabilities(state, a).flatten()\n",
    "                ########  Estimate value of action a\n",
    "                va = (next_state_prob * (REWARDS + sigma*v_old)).sum()\n",
    "                ########\n",
    "                if va > max_va:\n",
    "                    max_va = va\n",
    "            values[s] = max_va\n",
    "\n",
    "        delta = abs(sum(v_old - values))\n",
    "        #print(delta)\n",
    "    return values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33dcbb0d",
   "metadata": {},
   "source": [
    "estimate values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1dfa419c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "values = value_iteration(env)\n",
    "\n",
    "values_r = values.reshape((env.height, env.width))\n",
    "print(values_r)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09159f81",
   "metadata": {},
   "source": [
    "Estimate best action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f6a427b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_action(env, values, s):\n",
    "    max_va = 0\n",
    "    best_a = -1\n",
    "    for a in range(env.num_actions):\n",
    "        expected_value_a = 0\n",
    "        ###### Expected value a\n",
    "        \n",
    "        expected_value_a = env.transition_probabilities(s, a).flatten()*values).sum()\n",
    "        \n",
    "        ########\n",
    "\n",
    "        if expected_value_a > max_va:\n",
    "            max_va = expected_value_a\n",
    "            best_a = a\n",
    "    return best_a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a68ae40",
   "metadata": {},
   "source": [
    "simulate optimal policy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "edfe0c92",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RIGHT\n",
      "| A |___|///|\n",
      "|___|___|___|\n",
      "|___|///|///|\n",
      "|___|___|///|\n",
      "|___|___| G |\n",
      "\n",
      "\n",
      "RIGHT\n",
      "|___|___|///|\n",
      "|___| A |___|\n",
      "|___|///|///|\n",
      "|___|___|///|\n",
      "|___|___| G |\n",
      "\n",
      "\n",
      "RIGHT\n",
      "|___|___|///|\n",
      "|___| A |___|\n",
      "|___|///|///|\n",
      "|___|___|///|\n",
      "|___|___| G |\n",
      "\n",
      "\n",
      "RIGHT\n",
      "|___|___|///|\n",
      "|___|___| A |\n",
      "|___|///|///|\n",
      "|___|___|///|\n",
      "|___|___| G |\n",
      "\n",
      "\n",
      "RIGHT\n",
      "|___|___|///|\n",
      "|___|___| A |\n",
      "|___|///|///|\n",
      "|___|___|///|\n",
      "|___|___| G |\n",
      "\n",
      "\n",
      "RIGHT\n",
      "|___|___|///|\n",
      "|___|___| A |\n",
      "|___|///|///|\n",
      "|___|___|///|\n",
      "|___|___| G |\n",
      "\n",
      "\n",
      "RIGHT\n",
      "|___|___|///|\n",
      "|___|___| A |\n",
      "|___|///|///|\n",
      "|___|___|///|\n",
      "|___|___| G |\n",
      "\n",
      "\n",
      "RIGHT\n",
      "|___|___|///|\n",
      "|___|___| A |\n",
      "|___|///|///|\n",
      "|___|___|///|\n",
      "|___|___| G |\n",
      "\n",
      "\n",
      "RIGHT\n",
      "|___|___|///|\n",
      "|___|___| A |\n",
      "|___|///|///|\n",
      "|___|___|///|\n",
      "|___|___| G |\n",
      "\n",
      "\n",
      "RIGHT\n",
      "|___|___|///|\n",
      "|___|___| A |\n",
      "|___|///|///|\n",
      "|___|___|///|\n",
      "|___|___| G |\n",
      "\n",
      "\n",
      "RIGHT\n",
      "|___|___|///|\n",
      "|___|___| A |\n",
      "|___|///|///|\n",
      "|___|___|///|\n",
      "|___|___| G |\n",
      "\n",
      "\n",
      "RIGHT\n",
      "|___|___|///|\n",
      "|___|___| A |\n",
      "|___|///|///|\n",
      "|___|___|///|\n",
      "|___|___| G |\n",
      "\n",
      "\n",
      "RIGHT\n",
      "|___|___|///|\n",
      "|___|___| A |\n",
      "|___|///|///|\n",
      "|___|___|///|\n",
      "|___|___| G |\n",
      "\n",
      "\n",
      "RIGHT\n",
      "|___|___|///|\n",
      "|___|___| A |\n",
      "|___|///|///|\n",
      "|___|___|///|\n",
      "|___|___| G |\n",
      "\n",
      "\n",
      "RIGHT\n",
      "|___|___|///|\n",
      "|___|___| A |\n",
      "|___|///|///|\n",
      "|___|___|///|\n",
      "|___|___| G |\n",
      "\n",
      "\n",
      "RIGHT\n",
      "|___|___|///|\n",
      "|___|___| A |\n",
      "|___|///|///|\n",
      "|___|___|///|\n",
      "|___|___| G |\n",
      "\n",
      "\n",
      "RIGHT\n",
      "|___|___|///|\n",
      "|___|___| A |\n",
      "|___|///|///|\n",
      "|___|___|///|\n",
      "|___|___| G |\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "done = False\n",
    "state = env.reset()\n",
    "while not done:\n",
    "    action = best_action(env, values,state)\n",
    "    print(env.ACTION_NAMES[action])\n",
    "    state, reward, done, _ = env.step(action)\n",
    "    env.render()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
